{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# UdaciMed | Notebook 1: Baseline Analysis\n", "\n", "Welcome to UdaciMed! You are a Machine Learning Engineer tasked with optimizing our chest X-ray pneumonia detection model for production deployment.\n", "\n", "- **The challenge**: Before our new model can be approved for production, we must verify that it meets our strict performance SLAs. Deploying an unoptimized model could lead to high operational costs and poor performance across our diverse hardware fleet\u2014from shared T4 GPU cloud instances to portable clinic devices.\n", "\n", "- **Your mission**: Optimize the model to pass _UdaciMed's Universal Performance Standard_, a strict production service level agreement (SLA) that must be met using the universally compatible ONNX format on our standardized target device.\n", "\n", "### **Optimization goals**\n", "\n", "Your goal is to ensure the production model meets these production targets on our standardized development machine:\n", "\n", "- **< 0.4 GFLOPs per sample**: Floating Point Operations determine computational cost - reducing FLOPs is the most critical step toward broad-platform efficiency.\n", "- **< 100MB peak memory footprint**: Total memory consumption (parameters + activations + workspace) during inference - essential for running the model on memory-constrained edge devices and for enabling cost-effective multi-model environments in the cloud.\n", "- **< 3ms latency**: This ensures a real-time user experience. We will measure both *amortized latency* (average time per sample in a large batch) and *true latency* (time for a single-image inference), as both are important for different use cases.\n", "- **> 2000 samples/second throughput**: This specific target is for our high-performance hardware, like the reference T4 GPU. Meeting this goal proves the model is cost-effective and scalable for high-volume, server-side screening workflows.\n", "- **> 98% sensitivity***: This is a non-negotiable clinical safety requirement. We must ensure that a threshold percentage of all true pneumonia cases are correctly identified. All optimizations must be validated against this metric.\n", "\n", "#### **A note on our standardized target device**\n", "\n", "All performance targets in this project must be met on our official _\"standardized target device.\"_ This is an NVIDIA T4 GPU, a common and versatile datacenter GPU that represents a typical cloud deployment environment.\n", "\n", "By using a single, consistent hardware profile (NVIDIA T4 with 16GB VRAM, CUDA 12.4) for all our performance SLAs, we can:\n", "\n", "- *Ensure Reproducible Results*: Anyone on the team can validate performance and get consistent measurements.\n", "\n", "- *Create a Reliable Benchmark*: It provides a stable baseline to measure the impact of every optimization we make.\n", "\n", "If a model can meet our strict, universal performance standards on this reference hardware, we are confident it will perform well across our entire fleet of production devices.\n", "\n", "---\n", "\n", "Through this notebook, you will build the foundation for our optimization strategy by:\n", "\n", "1.  **Establishing baseline performance** with comprehensive profiling.\n", "2.  **Analyzing the primary bottlenecks**, distinguishing between compute, parameter memory, and activation memory.\n", "3.  **Identifying optimization opportunities** in both the model architecture and the deployment configuration.\n", "\n", "**Let's set up an optimization vision for UdaciMed's next-generation diagnostic platform!**\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_\\*Understanding medical AI requirements_**:\n", "> \n", "> In medical AI, sensitivity (recall) is often more critical than overall accuracy. Missing a pneumonia case (false negative) can be life-threatening, while a false positive \"only\" leads to additional human review. This is why we prioritize sensitivity as our safety constraint."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 1: Set up the environment\n", "The first step is to import all libraries and internal functionalities (from `utils`). \n", "\n", "Additionally, we set `pytorch` to use CUDA GPU if available (not only for faster execution, but also for benchmarking, as this will be our final deployment target!) and we include deterministic mode for reproducible benchmarking."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Make sure that libraries are dynamically re-loaded if changed\n", "%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import core libraries\n", "import torch\n", "import torch.nn as nn\n", "import inspect\n", "import numpy as np\n", "import os\n", "import pickle\n", "from pprint import pprint\n", "import random\n", "from torchsummary import summary\n", "import tqdm\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "# Import project utilities\n", "\n# Ensure project root is on sys.path so 'utils' imports work when running from the notebooks folder\n", "import sys, os\n", "if os.path.basename(os.getcwd()) == 'notebooks' and os.path.exists('..'):\n", "    sys.path.append('..')\n", "from utils.data_loader import (\n", "    load_pneumoniamnist, \n", "    get_dataset_info, \n", "    explore_dataset_splits,\n", "    visualize_sample_images,\n", "    get_sample_batch\n", ")\n", "from utils.model import (\n", "    create_baseline_model,\n", "    get_model_info,\n", "    count_parameters_by_type,\n", "    train_baseline_model,\n", "    plot_training_history\n", ")\n", "from utils.evaluation import (\n", "    evaluate_with_multiple_thresholds\n", ")\n", "from utils.profiling import (\n", "    PerformanceProfiler,\n", "    get_gpu_info,\n", "    check_environment,\n", "    measure_time\n", ")\n", "from utils.visualization import (\n", "    plot_dataset_distribution,\n", "    plot_performance_profile,\n", "    plot_operation_breakdown,\n", "    plot_batch_size_comparison,\n", ")\n", "\n", "# Check environment and GPU capabilities\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "print(f\"Using device: {device}\")\n", "\n", "if torch.cuda.is_available():\n", "    gpu_info = get_gpu_info()\n", "    print(f\"GPU: {gpu_info.get('name', 'Unknown')}\")\n", "    print(f\"GPU Memory: {gpu_info.get('memory_total_gb', 0):.1f} GB\")\n", "    print(f\"Tensor Core Support: {gpu_info.get('tensor_core_support', False)}\")\n", "else:\n", "    print(\"WARNING: CUDA not available - profiling will be limited\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Set random seed for reproducibility across optimization experiments\n", "def set_deterministic_mode(seed=42):\n", "    \"\"\"\n", "    Enable deterministic mode for consistent benchmarking.\n", "    Critical for fair comparison between different techniques.\n", "    \"\"\"\n", "    random.seed(seed)\n", "    np.random.seed(seed)\n", "    torch.manual_seed(seed)\n", "    torch.cuda.manual_seed_all(seed)\n", "    torch.backends.cudnn.deterministic = True\n", "    torch.backends.cudnn.benchmark = False  # Disable for consistent timing\n", "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n", "\n", "set_deterministic_mode(42)\n", "print(\"Deterministic mode enabled for reproducible benchmarking\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 2: Load and analyze the dataset\n", "Now, we can get started with our baseline model set-up by loading the data and understanding its characteristics. \n", "\n", "For this project, we use the PneumoniaMNIST dataset from [MedMNIST](https://medmnist.com/). PneumoniaMNIST provides a standardized, validated dataset for pneumonia detection research. Its 64x64 resolution balances clinical detail with computational efficiency, making it ideal for optimization studies while maintaining diagnostic relevance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get dataset information\n", "dataset_info = get_dataset_info(use_binary=True)\n", "print(\"PneumoniaMNIST Dataset Information:\")\n", "for key, value in dataset_info.items():\n", "    print(f\"   {key.replace('_', ' ').title()}: {value}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define configuration for baseline analysis\n", "CONFIG = {\n", "    'image_size': 64,  # Balanced for for memory usage and model accuracy\n", "    'num_classes': 2,  # Binary classification: normal vs pneumonia\n", "    'batch_size': 32,  # Balanced for memory usage and training stability\n", "    'subset_size': 800,  # Use a subset for faster execution during automated runs\n", "}\n", "\n", "# Load the dataset with optimized settings\n", "print(\"Loading PneumoniaMNIST dataset...\")\n", "\n", "with measure_time(\"Dataset loading\"):\n", "    train_loader = load_pneumoniamnist(\n", "        split=\"train\", download=True, \n", "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n", "        subset_size=int(CONFIG['subset_size'] * 0.7) if CONFIG['subset_size'] is not None else None\n", "    )\n", "    \n", "    val_loader = load_pneumoniamnist(\n", "        split=\"val\", download=False, \n", "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n", "        subset_size=int(CONFIG['subset_size'] * 0.15) if CONFIG['subset_size'] is not None else None\n", "    )\n", "    \n", "    test_loader = load_pneumoniamnist(\n", "        split=\"test\", download=False, \n", "        size=CONFIG['image_size'], batch_size=CONFIG['batch_size'], \n", "        subset_size=int(CONFIG['subset_size'] * 0.15) if CONFIG['subset_size'] is not None else None\n", "    )\n", "\n", "print(f\"Dataset loaded: {CONFIG['image_size']}x{CONFIG['image_size']} images, batch_size={CONFIG['batch_size']}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Analyze dataset distribution for class imbalance considerations\n", "print(\"Analyzing dataset distribution...\")\n", "dataset_splits = explore_dataset_splits(train_loader, val_loader, test_loader)\n", "print(f\"\\nDataset Summary: {dataset_splits}\")\n", "\n", "# Visualize dataset distribution\n", "plot_dataset_distribution(dataset_splits)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_The impact of class imbalance_**\n", "> \n", "> Medical datasets often have class imbalance. This affects optimization because:\n", "> \n", "> - Models may focus compute on majority class features\n", "> - Batch composition affects memory usage patterns\n", "> - Some optimization techniques (like pruning) may disproportionately affect minority class performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize sample images to understand data characteristics\n", "print(\"Sample chest X-ray images:\")\n", "visualize_sample_images(train_loader, num_samples=4)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 3: Create and analyze the baseline model\n", "We will use [ResNet-18](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) as our baseline - a popular choice for medical imaging that balances accuracy and efficiency.\n", "\n", "The original model structure is architected for ImageNet (1000 classes), so we modify the model with a custom head to support our new classification task."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create the baseline ResNet-18 model\n", "print(\"Creating ResNet-18 baseline model...\")\n", "\n", "baseline_model = create_baseline_model(\n", "    num_classes=CONFIG['num_classes'], \n", "    input_size=CONFIG['image_size'], \n", "    pretrained=False  # Training from scratch for fair optimization comparison\n", ")\n", "baseline_model = baseline_model.to(device)\n", "\n", "print(f\"Baseline model created and deployed to {device}\")\n", "print(f\"   Architecture: {baseline_model.architecture_name}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get model information\n", "model_info = get_model_info(baseline_model)\n", "\n", "print(f\"\\nModel Information:\")\n", "print(f\"   Architecture: {model_info['architecture']}\")\n", "print(f\"   Total Parameters: {model_info['total_parameters']:,}\")\n", "print(f\"   Model Size: {model_info['model_size_mb']:.1f} MB\")\n", "print(f\"   Input Size: {model_info['input_size']}x{model_info['input_size']}\")\n", "\n", "# Analyze layer composition \n", "layer_breakdown = model_info['layer_breakdown']\n", "print(f\"\\nLayer Composition:\")\n", "print(f\"   Convolution Layers: {layer_breakdown['conv_layers']['count']} ({layer_breakdown['conv_layers']['total_params']:,} params)\")\n", "print(f\"   Linear Layers: {layer_breakdown['linear_layers']['count']} ({layer_breakdown['linear_layers']['total_params']:,} params)\")\n", "print(f\"   Normalization Layers: {layer_breakdown['norm_layers']['count']}\")\n", "print(f\"   Activation Types: {', '.join(layer_breakdown['activation_layers']['types'])}\")\n", "\n", "# Get parameter distribution\n", "if 'parameter_distribution' in layer_breakdown:\n", "    param_dist = layer_breakdown['parameter_distribution']\n", "    print(f\"\\nParameter Distribution:\")\n", "    print(f\"   Convolution: {param_dist['conv_percentage']:.2f}%\")\n", "    print(f\"   Linear: {param_dist['linear_percentage']:.2f}%\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Display the model architecture\n", "summary(baseline_model, input_size=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"]))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_Brainstorming time: Optimizations by layer type_**\n", "> \n", "> Try to remember from the course, which architectural optimizations most benefit each layer type? Looking at the layer composition, our model is convolution-heavy - this is your starting point!"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 4: Train and evaluate baseline model\n", "Now, we define the baseline model and evaluate its performance on key accuracy metrics - all future optimizations should maintain a similar clinical performance standard.\n", "\n", "Establishing robust baseline metrics is crucial for medical AI. Any optimization must preserve clinical safety while improving computational efficiency."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training configuration\n", "TRAINING_CONFIG = {\n", "    'num_epochs': 3,            # Reduced for faster baseline execution\n", "    'learning_rate': 0.0003,    # Conservative rate for stable training\n", "    'lr_step_size': 3,          # Learning rate decay schedule\n", "    'weight_decay': 1e-4,       # Regularization for generalization\n", "    'patience': 1               # Early stopping for efficiency\n", "}\n", "\n", "# Train the model\n", "baseline_model, training_history = train_baseline_model(\n", "    baseline_model, train_loader, val_loader, device, TRAINING_CONFIG\n", ")\n", "\n", "# Plot training curves with analysis\n", "plot_training_history(training_history)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **Training insights: What does the training history tell us?**\n", "> \n", "> The initial low validation accuracy is due to extreme overfitting due to the small dataset size in comparison to the model's parameter size. The presence of early plateaus and fast convergence also highlight that the architecture has high representational power - we can likely apply aggressive optimization without accuracy degradation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate model performance with multiple thresholds \n", "print(f\"Running eval benchmark on {dataset_splits['test']['total']} test samples ...\")\n", "eval_results = evaluate_with_multiple_thresholds(baseline_model, test_loader, device, [0.4, 0.7])"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **Clinical threshold selection**\n", ">\n", "> Different thresholds optimize for different clinical scenarios. Lower thresholds (0.4) maximize sensitivity for screening, while higher thresholds (0.7) balance precision and recall for diagnostic confirmation."]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 5: Profile baseline model for latency, throughput, and memory usage\n", "\n", "Comprehensive performance profiling forms the foundation of our optimization strategy. We will measure all key metrics that impact multi-tenant deployment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize performance profiler\n", "profiler = PerformanceProfiler(device=str(device))\n", "print(f\"Performance profiler initialized for {device}\")\n", "\n", "# Get sample batch for profiling\n", "sample_images, sample_labels = get_sample_batch(val_loader)\n", "sample_images = sample_images.to(device)\n", "sample_labels = sample_labels.to(device)\n", "\n", "print(f\"\\nSample batch for profiling:\")\n", "print(f\"   Batch shape: {sample_images.shape}\")\n", "print(f\"   Memory usage: {sample_images.numel() * sample_images.element_size() / 1024**2:.1f} MB\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Profile inference timing\n", "print(\"Profiling inference timing...\")\n", "\n", "timing_results = profiler.profile_inference_time(\n", "    model=baseline_model,\n", "    input_tensor=sample_images,\n", "    num_runs=100,       # Sufficient for statistical significance\n", "    warmup_runs=10      # GPU warmup for consistent measurements\n", ")\n", "\n", "print(f\"\\nTiming Results:\")\n", "print(f\"   Single Sample Latency: {timing_results['single_sample_ms']:.2f} ms\")\n", "print(f\"   (Single) Batch Throughput: {timing_results['throughput_samples_per_sec']:.1f} samples/sec\")\n", "print(f\"   Batch Latency: {timing_results['batch_total_ms']:.2f} ms\")\n", "print(f\"   Batch Throughput: {timing_results['batch_throughput_samples_per_sec']:.1f} samples/sec\")\n", "print(f\"   Mean Inference Time: {timing_results['mean_ms']:.2f} ms\")\n", "print(f\"   95th Percentile: {timing_results['p95_ms']:.2f} ms\")\n", "print(f\"   Standard Deviation: {timing_results['std_ms']:.2f} ms\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **Latency vs throughput trade-offs**\n", ">\n", "> Single sample latency measures real-time diagnostic speed, while batch throughput indicates multi-tenant efficiency. Both metrics are crucial for different deployment scenarios."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Profile FLOPs for computational efficiency analysis\n", "flops_results = profiler.profile_flops(\n", "    model=baseline_model, \n", "    input_tensor=sample_images\n", ")\n", "\n", "if 'error' in flops_results:\n", "    print(f\"FLOPs calculation failed: {flops_results['error']}\")\n", "else:\n", "    print(f\"\\nFLOPs Results:\")\n", "    print(f\"   Total: {flops_results['total_gflops']:.2f} GFLOPs\")\n", "    print(f\"   Per Sample: {flops_results['gflops_per_sample']:.2f} GFLOPs\")\n", "    if 'module_percentage' in flops_results and flops_results['module_percentage']:\n", "        print(f\"\\n   Top Operations (by FLOPs):\")\n", "        for module, percentage in list(flops_results['module_percentage'].items())[:5]:\n", "            gflops = flops_results['module_breakdown_gflops'][module]\n", "            print(f\"     {module}: {percentage:.1f}% ({gflops:.2f} GFLOPs)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Profile GPU memory usage\n", "print(\"Profiling GPU memory usage...\")\n", "\n", "memory_results = profiler.profile_memory_usage(\n", "    model=baseline_model,\n", "    input_tensor=sample_images\n", ")\n", "\n", "if 'error' not in memory_results:\n", "    print(f\"\\nMemory Results:\")\n", "    print(f\"   Peak GPU Memory: {memory_results['peak_memory_mb']:.1f} MB\")\n", "    print(f\"   Memory Increase: {memory_results['memory_increase_mb']:.1f} MB\")\n", "    \n", "    # Component breakdown\n", "    if 'component_breakdown' in memory_results:\n", "        components = memory_results['component_breakdown']\n", "        print(f\"\\nMemory Component Breakdown:\")\n", "        for component, usage in components.items():\n", "            print(f\"   {component.replace('_', ' ').title()}: {usage:.1f} MB\")\n", "else:\n", "    print(f\"WARNING: Memory profiling error: {memory_results['error']}\")\n", "    memory_results = {}"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_Did you notice? A major optimization opportunity hiding in plain sight!_**\n", "> \n", "> Look carefully at the model summary above. Something doesn't add up with our input/output dimensions...\n", "> \n", "> Compare the input size we are feeding (64x64) with the first convolution layer's output size. The first Conv2d layer shows output `[-1, 64, 112, 112]` but our input is only 64x64. Where are those extra pixels coming from? Complete the TODO below to find out.\n", "> \n", "> *Optimization opportunity:* This might be your biggest single optimization win, in both speed and memory usage! Keep this insight in mind as you analyze the profiling results below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Inspect the baseline model's forward method using `inspect.get_source()`\n", "print(\"Manually inspect the ResNetBaseline forward method:\")\n", "baseline_model_forward = inspect.getsource(type(baseline_model).forward)\n", "print(baseline_model_forward)\n", "\n", "print(\"\\nDiscussion questions:\")\n", "print(\"1. What happens when height != self.target_size? What is self.target_size set to?\") \n", "print(\"2. How much computational and memory overhead does F.interpolate add? (Hint: Compare 64x64 vs 224x224 pixel counts)\")\n", "print(\"3. Is this interpolation necessary for pneumonia detection, or just a legacy from ImageNet pretraining?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Detailed PyTorch profiler\n", "print(\"Running detailed PyTorch profiler...\")\n", "\n", "detailed_results = profiler.profile_with_pytorch_profiler(\n", "    model=baseline_model,\n", "    input_tensor=sample_images,\n", "    num_steps=10        # Sufficient for operation breakdown analysis\n", ")\n", "\n", "if 'error' not in detailed_results:\n", "    print(f\"\\nOperation Breakdown:\")\n", "    op_breakdown = detailed_results['operation_breakdown']\n", "    \n", "    # Show top operations for optimization targeting\n", "    sorted_ops = sorted(op_breakdown.items(), key=lambda x: x[1], reverse=True)\n", "    for op_type, percentage in sorted_ops:\n", "        if percentage > 1:  # Only show operations > 1%\n", "            print(f\"   {op_type.replace('_', ' ').title()}: {percentage:.1f}%\")\n", "else:\n", "    print(f\"WARNING: Detailed profiling error: {detailed_results['error']}\")\n", "    detailed_results = {}"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Step 6: Visualize and save baseline model's performance\n", "\n", "Comprehensive visualization helps understand optimization opportunities and track progress across optimization experiments."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize performance profile\n", "plot_performance_profile(timing_results)\n", "\n", "# Visualize operation breakdown\n", "if detailed_results and 'operation_breakdown' in detailed_results:\n", "    plot_operation_breakdown(detailed_results['operation_breakdown'])\n", "else:\n", "    print(\"WARNING: Operation breakdown visualization not available\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compile baseline results for optimization notebooks\n", "baseline_results = {\n", "    'model_name': 'ResNet-18 Baseline',\n", "    'architecture': model_info['architecture'],\n", "    'total_parameters': model_info['total_parameters'],\n", "    'model_size_mb': model_info['model_size_mb'],\n", "    'config': CONFIG,\n", "    'eval_results': eval_results,\n", "    'timing': timing_results,\n", "    'flops': flops_results,\n", "    'memory': memory_results,\n", "    'operation_breakdown': detailed_results['operation_breakdown'],\n", "    'model_info': model_info,\n", "    'dataset_info': dataset_info,\n", "    'parameter_breakdown': count_parameters_by_type(baseline_model)\n", "}\n", "\n", "# Save baseline results\n", "with open('../results/baseline_results.pkl', 'wb') as f:\n", "    pickle.dump(baseline_results, f)\n", "\n", "print(\"Baseline results saved to 'baseline_results.pkl' in the `results/` folder\")\n", "print(\"   This will be used for comparison in optimization notebooks.\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **Brainstorming time!**\n", "> \n", "> Based on your profiling results above, analyze the following:\n", "> \n", "> 1. **Primary bottleneck**: What is the main performance bottleneck - compute time, memory usage, or data transfer?\n", "> \n", "> 2. **Operation analysis**: Which types of operations (convolution, linear, activation) consume the most time? What percentage?\n", "> \n", "> 3. **Memory patterns**: How much memory does the model use during inference? What contributes most to memory usage?\n", "> \n", "> 4. **Optimization priority**: Based on the profiling data, which optimization techniques would you prioritize:\n", ">    - Architecture modifications (channel reduction, efficient blocks)\n", ">    - Precision optimization (mixed precision, quantization)\n", ">    - Hardware acceleration (TensorRT for GPU, ...)\n", ">   \n", ">    _IMPORTANT:_ Did you discover the major inefficiency we hinted at earlier? How much improvement could removing the 64\u2192224 interpolation provide?"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## Step 7: Analyze optimization opportunities\n", "\n", "Now that we have established the baseline performance, it's time for you to conduct a deeper analysis that will guide the optimization strategy. This section contains **two focused analysis checkpoints**:\n", "\n", "1. **Architecture optimization analysis** - Identify specific opportunities in the ResNet-18 architecture\n", "2. **Deployment optimization analysis** - Understand hardware acceleration and deployment strategies\n", "\n", "Complete these analysis checkpoints to develop your optimization roadmap!"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Analysis checkpoint 1: Architecture optimization opportunities\n", "\n", "**Task:** Analyze the ResNet-18 architecture to identify the **top 2 optimization opportunities** from the techniques covered in the course. Available techniques to consider include:\n", "- Grouped convolutions\n", "- Depth-wise separable convolutions\n", "- Inverted residuals with linear bottlenecks\n", "- Low-rank factorization\n", "- Channel organization strategies _(NOTE: this is a hybrid optimization between architecture and hardware)_\n", "- Parameter sharing / weight tying\n", "\n", "Feel free to skip programmatic analysis of techniques which you deem to be non-applicable or less performant, but provide an explanation here or in the notebook's final markdown cell.\n", "\n", "**IMPORTANT:** Don't forget to also analyze the potential of interpolation removal from the model's forward method!\n", "\n", "#### Recommended strategic analysis approach\n", "\n", "Calculate the expected impact of applying each technique on parameter reduction programmatically to simplify follow-up analysis. Consider how parameter reduction / architectural improvements for each technique correlate with memory size (activation vs parameters), FLOPs, latency, throughput, and sensitivity to estimate optimization opportunity. HINTS are in each function's signature.\n", "\n", "To populate the analysis dictionary with estimated optimiation opportunity, you can either:\n", "1. Programmatically calculate optimization opportunity from parameter reduction and rule-of-thumb\n", "2. Directly add the expected value in the analysis entries, and add a one-line explanation of the value selected as an in-line comment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## Implement logic for each single technique analysis\n", "\n", "def _get_conv_coverage_from_profiler():\n", "    \"\"\"Return convolution time coverage (0-1) from detailed_results, robust to key naming.\"\"\"\n", "    try:\n", "        ops = detailed_results.get('operation_breakdown', {})\n", "        conv_pct = 0.0\n", "        for k, v in ops.items():\n", "            kl = k.replace('_', ' ').lower()\n", "            if 'convolution' in kl or 'conv' in kl:\n", "                conv_pct += float(v)\n", "        return conv_pct / 100.0 if conv_pct > 0 else 0.0\n", "    except Exception:\n", "        return 0.0\n", "\n", "def analyze_grouped_conv_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"]), groups=2):\n", "    \"\"\"\n", "    Identify convolution layers suitable for grouped convolutions and estimate impact.\n", "    Assumptions: FLOPs scale ~ linearly with parameter count per conv; overall\n", "    speedup estimated via Amdahl's law using conv coverage from the profiler.\n", "    \"\"\"\n", "    import torch.nn as nn\n", "\n", "    conv_total_params = 0\n", "    candidate_params = 0\n", "    candidates = []\n", "\n", "    for name, m in model.named_modules():\n", "        if isinstance(m, nn.Conv2d) and m.kernel_size[0] > 1 and m.groups == 1:\n", "            params = m.in_channels * m.out_channels * (m.kernel_size[0] ** 2)\n", "            conv_total_params += params\n", "            if m.in_channels % groups == 0 and m.out_channels % groups == 0:\n", "                candidate_params += params\n", "                candidates.append({\n", "                    'name': name,\n", "                    'in': m.in_channels,\n", "                    'out': m.out_channels,\n", "                    'k': m.kernel_size[0],\n", "                    'stride': m.stride[0]\n", "                })\n", "\n", "    if conv_total_params == 0:\n", "        conv_total_params = 1\n", "\n", "    # Param/FLOP reduction on candidates\n", "    reduction_ratio_layer = 1.0 - (1.0 / groups)\n", "    param_reduction_ratio_overall = (candidate_params / conv_total_params) * reduction_ratio_layer\n", "\n", "    conv_cov = _get_conv_coverage_from_profiler() or 0.88\n", "    candidate_cov = candidate_params / conv_total_params\n", "\n", "    # Amdahl\u2019s law across conv + non-conv fractions\n", "    new_time = (1 - conv_cov) + conv_cov * (candidate_cov * (1 / groups) + (1 - candidate_cov) * 1.0)\n", "    speedup = 1.0 / new_time if new_time > 0 else 1.0\n", "\n", "    # Parameter memory reduction (MB)\n", "    saved_params = candidate_params * reduction_ratio_layer\n", "    saved_mb = (saved_params * 4) / (1024 ** 2)\n", "\n", "    base_throughput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    est_throughput = base_throughput * speedup\n", "\n", "    analysis = {\n", "        'technique': 'Grouped Convolutions',\n", "        'groups': groups,\n", "        'candidate_layers': candidates,\n", "        'total_candidates': len(candidates),\n", "        'avg_flop_reduction_percent': round(param_reduction_ratio_overall * 100, 2),\n", "        'avg_param_reduction_percent': round(param_reduction_ratio_overall * 100, 2),\n", "        'estimated_speedup': round(speedup, 3),\n", "        'estimated_memory_reduction_mb': round(saved_mb, 2),\n", "        'estimated_throughput_samples_sec': round(est_throughput, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Moderate (may reduce cross-channel mixing; validate sensitivity)'\n", "    }\n", "\n", "    return analysis\n", "\n", "def analyze_depthwise_separable_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n", "    \"\"\"\n", "    Estimate impact of replacing eligible 3\u00d73 convs with depthwise-separable convs.\n", "    Separable params = k^2*in + in*out. Standard = k^2*in*out.\n", "    \"\"\"\n", "    import torch.nn as nn\n", "\n", "    conv_total_params = 0\n", "    separable_params_total = 0\n", "    standard_params_total = 0\n", "    candidates = []\n", "\n", "    for name, m in model.named_modules():\n", "        if isinstance(m, nn.Conv2d) and m.kernel_size[0] > 1 and m.groups == 1 and m.in_channels >= 16:\n", "            k = m.kernel_size[0]\n", "            std = m.in_channels * m.out_channels * (k ** 2)\n", "            sep = (m.in_channels * (k ** 2)) + (m.in_channels * m.out_channels)\n", "            standard_params_total += std\n", "            separable_params_total += sep\n", "            candidates.append({'name': name, 'in': m.in_channels, 'out': m.out_channels, 'k': k, 'stride': m.stride[0]})\n", "        if isinstance(m, nn.Conv2d) and m.kernel_size[0] > 1:\n", "            conv_total_params += m.in_channels * m.out_channels * (m.kernel_size[0] ** 2)\n", "\n", "    if conv_total_params == 0 or standard_params_total == 0:\n", "        speedup = 1.0\n", "        avg_reduction = 0.0\n", "        saved_mb = 0.0\n", "    else:\n", "        # Overall conv time fraction and candidate coverage\n", "        conv_cov = _get_conv_coverage_from_profiler() or 0.88\n", "        candidate_cov = standard_params_total / conv_total_params\n", "\n", "        # Conv time factor after separable substitution on candidates\n", "        conv_factor = candidate_cov * (separable_params_total / standard_params_total) + (1 - candidate_cov) * 1.0\n", "        theoretical_speedup = 1.0 / ((1 - conv_cov) + conv_cov * conv_factor)\n", "        # Penalize for memory-access overhead (empirical)\n", "        speedup = 1.0 + (theoretical_speedup - 1.0) * 0.6\n", "\n", "        avg_reduction = 1.0 - (separable_params_total / standard_params_total)\n", "        saved_params = (standard_params_total - separable_params_total)\n", "        saved_mb = (saved_params * 4) / (1024 ** 2)\n", "\n", "    base_throughput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    est_throughput = base_throughput * speedup\n", "\n", "    analysis = {\n", "        'technique': 'Depthwise Separable Convolutions',\n", "        'candidate_layers': candidates,\n", "        'total_candidates': len(candidates),\n", "        'avg_flop_reduction_percent': round(avg_reduction * 100, 2),\n", "        'avg_param_reduction_percent': round(avg_reduction * 100, 2),\n", "        'estimated_speedup': round(speedup, 3),\n", "        'estimated_memory_reduction_mb': round(saved_mb, 2),\n", "        'estimated_throughput_samples_sec': round(est_throughput, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Moderate (structure change; retrain/validate)'\n", "    }\n", "\n", "    return analysis\n", "\n", "def analyze_inverted_residuals_potential(model):\n", "    \"\"\"\n", "    Heuristic: consider stride-2 transition blocks (where channels double)\n", "    as candidates for MobileNetV2-style inverted residuals.\n", "    \"\"\"\n", "    import torch.nn as nn\n", "    conv_total_params = 0\n", "    trans_params = 0\n", "    trans_layers = []\n", "    for name, m in model.named_modules():\n", "        if isinstance(m, nn.Conv2d) and m.kernel_size[0] > 1:\n", "            conv_total_params += m.in_channels * m.out_channels * (m.kernel_size[0] ** 2)\n", "            if m.stride[0] == 2 and m.groups == 1:\n", "                trans_params += m.in_channels * m.out_channels * (m.kernel_size[0] ** 2)\n", "                trans_layers.append({'name': name, 'in': m.in_channels, 'out': m.out_channels, 'k': m.kernel_size[0]})\n", "\n", "    conv_cov = _get_conv_coverage_from_profiler() or 0.88\n", "    candidate_cov = (trans_params / conv_total_params) if conv_total_params else 0.0\n", "    # Assume ~1.2x faster on those transitions after IR blocks\n", "    conv_factor = candidate_cov * (1 / 1.2) + (1 - candidate_cov) * 1.0\n", "    new_time = (1 - conv_cov) + conv_cov * conv_factor\n", "    speedup = 1.0 / new_time if new_time > 0 else 1.0\n", "\n", "    analysis = {\n", "        'technique': 'Inverted Residuals',\n", "        'transition_layers': trans_layers,\n", "        'total_candidates': len(trans_layers),\n", "        'estimated_speedup': round(speedup, 3),\n", "        'avg_flop_reduction_percent': round((1 - conv_factor) * conv_cov * 100, 2),\n", "        'estimated_memory_reduction_mb': 0.0,\n", "        'estimated_throughput_samples_sec': round(timing_results.get('batch_throughput_samples_per_sec', 0) * speedup, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Low\u2011Moderate (structure change in downsampling blocks)'\n", "    }\n", "    return analysis\n", "\n", "def analyze_lowrank_factorization_potential(model, batch_size=32):\n", "    \"\"\"\n", "    Lightweight estimate for low\u2011rank factorization potential (linear layers).\n", "    ResNet\u201118 has a tiny classifier (512\u21922), so impact is negligible.\n", "    \"\"\"\n", "    import torch.nn as nn\n", "    total_linear = 0\n", "    linear_layers = 0\n", "    saved = 0\n", "    for m in model.modules():\n", "        if isinstance(m, nn.Linear):\n", "            linear_layers += 1\n", "            params = m.in_features * m.out_features\n", "            total_linear += params\n", "            # hypothetical rank\u2011reduction to 1/4\n", "            r = max(1, min(m.in_features, m.out_features) // 4)\n", "            sep = m.in_features * r + r * m.out_features\n", "            saved += max(0, params - sep)\n", "    total_params = sum(p.numel() for p in model.parameters())\n", "    saved_mb = (saved * 4) / (1024 ** 2)\n", "    speedup = 1.0 + min(0.02, saved / max(1, total_params))  # tiny effect\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', 0)\n", "    return {\n", "        'technique': 'Low\u2011Rank Factorization (FC)',\n", "        'total_candidates': linear_layers,\n", "        'avg_param_reduction_percent': round((saved / max(1, total_linear)) * 100, 3) if total_linear else 0.0,\n", "        'estimated_speedup': round(speedup, 3),\n", "        'avg_flop_reduction_percent': round((saved / max(1, total_params)) * 100, 3),\n", "        'estimated_memory_reduction_mb': round(saved_mb, 3),\n", "        'estimated_throughput_samples_sec': round(base_tput * speedup, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 2),\n", "        'sensitivity_risk': 'Low (classifier only)'\n", "    }\n", "\n", "def analyze_channel_organization_potential(model):\n", "    \"\"\"Estimate win from channels_last + in\u2011place activations (no FLOP change).\"\"\"\n", "    import torch.nn as nn\n", "    relu_not_inplace = sum(1 for m in model.modules() if isinstance(m, nn.ReLU) and not m.inplace)\n", "    base_speedup = 1.08  # conservative channels_last benefit\n", "    speedup = base_speedup * (1.0 + min(relu_not_inplace, 20) * 0.008)\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    analysis = {\n", "        'technique': 'Channel Organization (channels_last + in-place ReLU)',\n", "        'inplace_relu_candidates': relu_not_inplace,\n", "        'inplace_opportunities': relu_not_inplace,\n", "        'estimated_speedup': round(speedup, 3),\n", "        'estimated_memory_reduction_mb': 0,\n", "        'avg_flop_reduction_percent': 0,\n", "        'estimated_throughput_samples_sec': round(base_tput * speedup, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Low (no numerical change expected)'\n", "    }\n", "    return analysis\n", "\n", "def analyze_parameter_sharing_potential(model):\n", "    \"\"\"Rough estimate of weight sharing potential across identical conv shapes.\"\"\"\n", "    import torch.nn as nn\n", "    from collections import defaultdict\n", "    shape_groups = defaultdict(list)\n", "    total_params = 0\n", "    for name, m in model.named_modules():\n", "        if isinstance(m, nn.Conv2d) and m.groups == 1:\n", "            k = m.kernel_size[0]\n", "            params = m.in_channels * m.out_channels * (k ** 2)\n", "            total_params += params\n", "            key = (m.in_channels, m.out_channels, k)\n", "            shape_groups[key].append((name, params))\n", "    shareable = 0\n", "    groups_summary = {}\n", "    for key, items in shape_groups.items():\n", "        if len(items) > 1:\n", "            saved = sum(p for _, p in items[1:])\n", "            shareable += saved\n", "            groups_summary[str(key)] = len(items)\n", "    sharing_potential = min(0.25, (shareable / total_params) if total_params else 0.0)\n", "    speedup = 1.0 + sharing_potential * 0.4\n", "    saved_mb = (shareable * 4) / (1024 ** 2)\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', 0)\n", "    analysis = {\n", "        'technique': 'Parameter Sharing',\n", "        'similar_layer_groups': groups_summary,\n", "        'sharing_potential_percent': round(sharing_potential * 100, 2),\n", "        'avg_flop_reduction_percent': round(sharing_potential * 100, 2),\n", "        'estimated_speedup': round(speedup, 3),\n", "        'estimated_memory_reduction_mb': round(saved_mb, 2),\n", "        'estimated_throughput_samples_sec': round(base_tput * speedup, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Medium (may constrain representation; requires retraining)'\n", "    }\n", "    return analysis\n", "\n", "def analyze_interpolation_removal_potential(model, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n", "    \"\"\"Quantify benefit of removing 64\u2192224 interpolation inside forward().\"\"\"\n", "    target = getattr(model, 'target_size', 224)\n", "    native = CONFIG[\"image_size\"]\n", "    factor = (target / native) ** 2 if native else 1.0\n", "    conv_cov = _get_conv_coverage_from_profiler() or 0.88\n", "    scalable_portion = conv_cov * 0.6  # heuristic from instructions\n", "    fixed_portion = 1.0 - scalable_portion\n", "    new_time = fixed_portion + scalable_portion / factor\n", "    speedup = 1.0 / new_time if new_time > 0 else 1.0\n", "    flop_reduction_pct = (1.0 - 1.0 / factor) * conv_cov * 100\n", "\n", "    # Estimate activation memory savings\n", "    comp = (memory_results or {}).get('component_breakdown', {}) if isinstance(memory_results, dict) else {}\n", "    act_key = 'estimated_activations_mb' if 'estimated_activations_mb' in comp else ('activations_mb' if 'activations_mb' in comp else None)\n", "    saved_mb = 0.0\n", "    if act_key:\n", "        saved_mb = comp[act_key] * (1.0 - 1.0 / factor)\n", "\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    analysis = {\n", "        'technique': 'Interpolation Removal (Native Resolution)',\n", "        'interpolation_size': int(target),\n", "        'original_image_size': native,\n", "        'avg_flop_reduction_percent': round(flop_reduction_pct, 2),\n", "        'estimated_speedup': round(speedup, 3),\n", "        'estimated_memory_reduction_mb': round(saved_mb, 2),\n", "        'estimated_throughput_samples_sec': round(base_tput * speedup, 1),\n", "        'throughput_improvement_percent': round((speedup - 1) * 100, 1),\n", "        'sensitivity_risk': 'Low\u2011Moderate (must validate at native resolution)'\n", "    }\n", "\n", "    return analysis\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run the complete architecture optimization analysis with all 5 implemented analysis functions above\n", "def run_architecture_optimization_analysis(model):\n", "    \"\"\"\n", "    Main function to run all architecture optimization analyses.\n", "    \"\"\"\n", "    print(\"Running architecture optimization analysis...\")\n", "    \n", "    # Call each analysis function\n", "    depthwise_analysis = analyze_depthwise_separable_potential(model)\n", "    grouped_analysis = analyze_grouped_conv_potential(model)\n", "    inverted_analysis = analyze_inverted_residuals_potential(model)\n", "    lowrank_analysis = analyze_lowrank_factorization_potential(model)\n", "    channel_analysis = analyze_channel_organization_potential(model)\n", "    sharing_analysis = analyze_parameter_sharing_potential(model)\n", "    interpolation_analysis = analyze_interpolation_removal_potential(model)\n", "    \n", "    # Combine all analyses into comprehensive result\n", "    # TODO: Remove any techniques which haven't been analyzed\n", "    optimization_techniques = {\n", "        'grouped_convolutions': {\n", "            'candidate_layers': grouped_analysis['total_candidates'],\n", "            'groups': grouped_analysis['groups'],\n", "            'avg_param_reduction_percent': grouped_analysis['avg_param_reduction_percent'],\n", "            'estimated_speedup': grouped_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': grouped_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': grouped_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': grouped_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': grouped_analysis['sensitivity_risk'],\n", "            'details': grouped_analysis['candidate_layers'][:3]  # Show top 3\n", "        },\n", "        'depthwise_separable': {\n", "            'candidate_layers': depthwise_analysis['total_candidates'],\n", "            'avg_param_reduction_percent': depthwise_analysis['avg_param_reduction_percent'],\n", "            'estimated_speedup': depthwise_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': depthwise_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': depthwise_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': depthwise_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': depthwise_analysis['sensitivity_risk'],\n", "            'details': depthwise_analysis['candidate_layers'][:3]  # Show top 3\n", "        },\n", "        'inverted_residuals': {\n", "            'expansion_candidates': inverted_analysis['total_candidates'],\n", "            'estimated_speedup': inverted_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': inverted_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': inverted_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': inverted_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': inverted_analysis['sensitivity_risk']\n", "        },\n", "        'low_rank_factorization': {\n", "            'candidate_layers': lowrank_analysis['total_candidates'],\n", "            'avg_param_reduction_percent': lowrank_analysis['avg_param_reduction_percent'],\n", "            'estimated_speedup': lowrank_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': lowrank_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': lowrank_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': lowrank_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': lowrank_analysis['sensitivity_risk']\n", "        },\n", "        'channel_organization': {\n", "            'inplace_opportunities': channel_analysis['inplace_opportunities'],\n", "            'estimated_speedup': channel_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': channel_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': channel_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': channel_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': channel_analysis['sensitivity_risk']\n", "        },\n", "        'parameter_sharing': {\n", "            'sharing_potential_percent': sharing_analysis['sharing_potential_percent'],\n", "            'estimated_speedup': sharing_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': sharing_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': sharing_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': sharing_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': sharing_analysis['sensitivity_risk']\n", "        },\n", "        'interpolation_removal': {\n", "            'current_input_size': interpolation_analysis[\"interpolation_size\"],\n", "            'native_input_size': interpolation_analysis[\"original_image_size\"],\n", "            'estimated_speedup': interpolation_analysis['estimated_speedup'],\n", "            'avg_flop_reduction_percent': interpolation_analysis['avg_flop_reduction_percent'],\n", "            'estimated_memory_reduction_mb': interpolation_analysis['estimated_memory_reduction_mb'],\n", "            'throughput_improvement_percent': interpolation_analysis['throughput_improvement_percent'],\n", "            'sensitivity_risk': interpolation_analysis['sensitivity_risk']\n", "        }\n", "    }\n", "    \n", "    return optimization_techniques\n", "\n", "# Execute architecture analysis\n", "arch_analysis = run_architecture_optimization_analysis(baseline_model)\n", "\n", "# Display results\n", "print(f\"\\nARCHITECTURE OPTIMIZATION RESULTS:\")\n", "for technique, details in arch_analysis.items():\n", "    print(f\"\\n   {technique.replace('_', ' ').title()}:\")\n", "    for key, value in details.items():\n", "        if key != 'details':\n", "            print(f\"     {key.replace('_', ' ').title()}: {value}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Summarize architecture techniques quantitatively\n", "import pandas as pd\n", "rows=[]\n", "for name, d in arch_analysis.items():\n", "    # robust candidate counting across techniques\n", "    cand = 0\n", "    if isinstance(d.get('candidate_layers'), list):\n", "        cand = len(d.get('candidate_layers'))\n", "    elif isinstance(d.get('transition_layers'), list):\n", "        cand = len(d.get('transition_layers'))\n", "    elif isinstance(d.get('expansion_candidates'), (int, float)):\n", "        cand = int(d.get('expansion_candidates'))\n", "    elif isinstance(d.get('inplace_opportunities'), (int, float)):\n", "        cand = int(d.get('inplace_opportunities'))\n", "\n", "    rows.append({\n", "        'technique': name,\n", "        'candidates': cand,\n", "        'avg_param_reduction_%': d.get('avg_param_reduction_percent', 0),\n", "        'avg_FLOP_reduction_%': d.get('avg_flop_reduction_percent', 0),\n", "        'estimated_speedup_x': d.get('estimated_speedup', 1.0),\n", "        'throughput_improvement_%': d.get('throughput_improvement_percent', 0),\n", "        'mem_savings_MB': d.get('estimated_memory_reduction_mb', 0),\n", "        'risk': d.get('sensitivity_risk','')\n", "    })\n", "arch_df=pd.DataFrame(rows)\n", "arch_df.sort_values(by='estimated_speedup_x', ascending=False, inplace=True)\n", "arch_df.reset_index(drop=True, inplace=True)\n", "arch_df\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_Before you move on...brainstorming time!_**\n", "> \n", "> Based on your architecture analysis results, collect yours insights on architectural opportunities\n", "> \n", "> 1. **Primary bottleneck**: What operation type consumes the most compute time in your model? Why does this pattern make sense for ResNet-18 architecture? How does this inform your optimization strategy?\n", "> \n", "> 2. **Resource trade-offs:**: Which optimizations reduce parameters vs which improve compute efficiency? How might accuracy be affected by each technique?\n", "> \n", "> 3. **Optimization priority**: In which order would you recommend implementing the 5 architectural techniques? Consider both impact and implementation difficulty.\n", "> \n", "> 4. **Feasibility assessment:**: Will the combined optimizations achieve the 3ms target? If not, what additional techniques might be needed?"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Implement logic for each single technique analysis \n", "\n", "def analyze_mixed_precision_potential(detailed_results):\n", "    \"\"\"\n", "    Analyze mixed-precision (FP16) potential using profiler op breakdown + simple heuristics.\n", "    Returns a dict with coverage, speedup estimate, memory reduction and throughput impact.\n", "    \"\"\"\n", "    ops = (detailed_results or {}).get('operation_breakdown', {})\n", "    # Sum percentages of conv/matmul-like ops as tensor-core eligible\n", "    eligible = 0.0\n", "    total = 0.0\n", "    for k, v in ops.items():\n", "        total += float(v)\n", "        kl = k.replace('_', ' ').lower()\n", "        if any(t in kl for t in ['convolution', 'conv', 'matmul', 'gemm', 'linear']):\n", "            eligible += float(v)\n", "    coverage = (eligible / total * 100.0) if total > 0 else 70.0  # default if missing\n", "\n", "    # Speedup model: baseline 1.5x; if coverage >50%, add small slope toward 2.5x\n", "    if coverage >= 50:\n", "        estimated_speedup = 1.8 + (coverage - 50) * 0.014  # 50%\u21921.8x, 100%\u21922.5x\n", "    else:\n", "        estimated_speedup = 1.5\n", "    estimated_speedup = float(max(1.0, min(2.5, estimated_speedup)))\n", "\n", "    # Memory savings: FP32\u2192FP16 ~50% for activations + params\n", "    peak_mb = 0.0\n", "    if isinstance(memory_results, dict):\n", "        peak_mb = float(memory_results.get('peak_memory_mb', 0.0))\n", "    mem_savings = 0.5 * peak_mb if peak_mb else 0.0\n", "\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    throughput = base_tput * estimated_speedup if base_tput else 0\n", "\n", "    return {\n", "        'technique': 'Mixed Precision (FP16)',\n", "        'mixed_precision_coverage_percent': float(coverage),\n", "        'avg_flop_reduction_percent': float(min(50.0, coverage)),\n", "        'estimated_speedup': float(estimated_speedup),\n", "        'estimated_memory_reduction_mb': float(mem_savings),\n", "        'estimated_throughput_samples_sec': float(throughput),\n", "        'throughput_improvement_percent': float((estimated_speedup - 1) * 100),\n", "        'sensitivity_risk': 'Low\u2013Moderate (validate numerics and calibration)'\n", "    }\n", "\n", "def analyze_batch_processing_scenarios(model, mixed_precision_speedup, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n", "    \"\"\"\n", "    Profile multiple batch sizes and propose optimal configs for real-time and throughput.\n", "    Uses the global `profiler` created earlier and the observed `sample_images` shape.\n", "    \"\"\"\n", "    # Choose a reasonable range up to the observed sample batch size\n", "    max_bs = int(sample_images.shape[0]) if 'sample_images' in globals() else 32\n", "    batch_sizes = [b for b in [1, 2, 4, 8, 16, 32] if b <= max_bs]\n", "\n", "    print(\"   Profiling multiple batch sizes...\")\n", "    batch_results = profiler.profile_multiple_batch_sizes(\n", "        model, sample_images.shape, batch_sizes\n", "    )\n", "\n", "    # Parse results and pick optimal batch sizes\n", "    valid = {}\n", "    for k, v in batch_results.items():\n", "        if isinstance(v, dict) and 'timing' in v:\n", "            t = v['timing']\n", "            per_sample = t.get('batch_per_sample_ms', t.get('single_sample_ms', None))\n", "            throughput = t.get('batch_throughput_samples_per_sec', t.get('throughput_samples_per_sec', None))\n", "            if per_sample is not None and throughput is not None:\n", "                valid[int(k.split('_')[1])] = {\n", "                    'latency_ms': float(per_sample),\n", "                    'throughput': float(throughput)\n", "                }\n", "\n", "    scenarios = {\n", "        'real_time_diagnosis': {\n", "            'optimal_batch_size': None,\n", "            'current_latency_ms': None,\n", "            'mixed_precision_latency_ms': None,\n", "            'use_case': 'Emergency diagnosis, single patient processing'\n", "        },\n", "        'batch_processing': {\n", "            'optimal_batch_size': None,\n", "            'current_throughput_samples_sec': None,\n", "            'mixed_precision_throughput_samples_sec': None,\n", "            'use_case': 'Screening workflows, research processing'\n", "        }\n", "    }\n", "\n", "    if valid:\n", "        # minimize latency per sample for real-time\n", "        rt_bs = min(valid.keys(), key=lambda b: valid[b]['latency_ms'])\n", "        rt_lat = valid[rt_bs]['latency_ms']\n", "        scenarios['real_time_diagnosis'].update({\n", "            'optimal_batch_size': rt_bs,\n", "            'current_latency_ms': round(rt_lat, 2),\n", "            'mixed_precision_latency_ms': round(rt_lat / mixed_precision_speedup, 2)\n", "        })\n", "        # maximize throughput for batch processing\n", "        th_bs = max(valid.keys(), key=lambda b: valid[b]['throughput'])\n", "        th_tp = valid[th_bs]['throughput']\n", "        scenarios['batch_processing'].update({\n", "            'optimal_batch_size': th_bs,\n", "            'current_throughput_samples_sec': round(th_tp, 1),\n", "            'mixed_precision_throughput_samples_sec': round(th_tp * mixed_precision_speedup, 1)\n", "        })\n", "\n", "    if not valid:\n", "        # Fallback using existing timing_results\n", "        rt_lat = float(timing_results.get('single_sample_ms', timing_results.get('mean_ms', 0)))\n", "        base_tp = float(timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0)))\n", "        scenarios['real_time_diagnosis'].update({\n", "            'optimal_batch_size': 1,\n", "            'current_latency_ms': round(rt_lat, 2) if rt_lat else None,\n", "            'mixed_precision_latency_ms': round(rt_lat / mixed_precision_speedup, 2) if rt_lat else None\n", "        })\n", "        scenarios['batch_processing'].update({\n", "            'optimal_batch_size': timing_results.get('batch_size', 1),\n", "            'current_throughput_samples_sec': round(base_tp, 1) if base_tp else None,\n", "            'mixed_precision_throughput_samples_sec': round(base_tp * mixed_precision_speedup, 1) if base_tp else None\n", "        })\n", "    return batch_results, scenarios\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["### Implement logic for each single technique analysis \n", "\n", "def analyze_mixed_precision_potential(detailed_results):\n", "    \"\"\"\n", "    Analyze mixed-precision (FP16) potential using profiler op breakdown + simple heuristics.\n", "    Returns a dict with coverage, speedup estimate, memory reduction and throughput impact.\n", "    \"\"\"\n", "    ops = (detailed_results or {}).get('operation_breakdown', {})\n", "    eligible = 0.0\n", "    total = 0.0\n", "    for k, v in ops.items():\n", "        total += float(v)\n", "        kl = k.replace('_', ' ').lower()\n", "        if any(t in kl for t in ['convolution', 'conv', 'matmul', 'gemm', 'linear']):\n", "            eligible += float(v)\n", "    coverage = (eligible / total * 100.0) if total > 0 else ( (_get_conv_coverage_from_profiler()*100.0) if 'detailed_results' in globals() else 70.0 )\n", "\n", "    if coverage >= 50:\n", "        estimated_speedup = 1.8 + (coverage - 50.0) * 0.014\n", "    else:\n", "        estimated_speedup = 1.5\n", "    estimated_speedup = float(max(1.0, min(2.5, estimated_speedup)))\n", "\n", "    peak_mb = 0.0\n", "    if isinstance(memory_results, dict):\n", "        peak_mb = float(memory_results.get('peak_memory_mb', 0.0))\n", "    mem_savings = 0.5 * peak_mb if peak_mb else 0.0\n", "\n", "    base_tput = timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0))\n", "    throughput = base_tput * estimated_speedup if base_tput else 0\n", "\n", "    return {\n", "        'technique': 'Mixed Precision (FP16)',\n", "        'mixed_precision_coverage_percent': float(coverage),\n", "        'avg_flop_reduction_percent': float(min(50.0, coverage)),\n", "        'estimated_speedup': float(estimated_speedup),\n", "        'estimated_memory_reduction_mb': float(mem_savings),\n", "        'estimated_throughput_samples_sec': float(throughput),\n", "        'throughput_improvement_percent': float((estimated_speedup - 1) * 100),\n", "        'sensitivity_risk': 'Low\u2013Moderate (validate numerics and calibration)'\n", "    }\n", "\n", "def analyze_batch_processing_scenarios(model, mixed_precision_speedup, sample_input_shape=(3, CONFIG[\"image_size\"], CONFIG[\"image_size\"])):\n", "    \"\"\"\n", "    Profile multiple batch sizes and propose optimal configs for real-time and throughput.\n", "    Uses the global `profiler` created earlier and the observed `sample_images` shape.\n", "    \"\"\"\n", "    max_bs = int(sample_images.shape[0]) if 'sample_images' in globals() else 32\n", "    batch_sizes = [b for b in [1, 2, 4, 8, 16, 32] if b <= max_bs]\n", "\n", "    print(\"   Profiling multiple batch sizes...\")\n", "    batch_results = profiler.profile_multiple_batch_sizes(\n", "        model, sample_images.shape, batch_sizes\n", "    )\n", "\n", "    valid = {}\n", "    for k, v in batch_results.items():\n", "        if isinstance(v, dict) and 'timing' in v:\n", "            t = v['timing']\n", "            per_sample = t.get('batch_per_sample_ms', t.get('single_sample_ms', None))\n", "            throughput = t.get('batch_throughput_samples_per_sec', t.get('throughput_samples_per_sec', None))\n", "            if per_sample is not None and throughput is not None:\n", "                valid[int(k.split('_')[1])] = {\n", "                    'latency_ms': float(per_sample),\n", "                    'throughput': float(throughput)\n", "                }\n", "\n", "    scenarios = {\n", "        'real_time_diagnosis': {\n", "            'optimal_batch_size': None,\n", "            'current_latency_ms': None,\n", "            'mixed_precision_latency_ms': None,\n", "            'use_case': 'Emergency diagnosis, single patient processing'\n", "        },\n", "        'batch_processing': {\n", "            'optimal_batch_size': None,\n", "            'current_throughput_samples_sec': None,\n", "            'mixed_precision_throughput_samples_sec': None,\n", "            'use_case': 'Screening workflows, research processing'\n", "        }\n", "    }\n", "\n", "    if valid:\n", "        rt_bs = min(valid.keys(), key=lambda b: valid[b]['latency_ms'])\n", "        rt_lat = valid[rt_bs]['latency_ms']\n", "        scenarios['real_time_diagnosis'].update({\n", "            'optimal_batch_size': rt_bs,\n", "            'current_latency_ms': round(rt_lat, 2),\n", "            'mixed_precision_latency_ms': round(rt_lat / mixed_precision_speedup, 2)\n", "        })\n", "        th_bs = max(valid.keys(), key=lambda b: valid[b]['throughput'])\n", "        th_tp = valid[th_bs]['throughput']\n", "        scenarios['batch_processing'].update({\n", "            'optimal_batch_size': th_bs,\n", "            'current_throughput_samples_sec': round(th_tp, 1),\n", "            'mixed_precision_throughput_samples_sec': round(th_tp * mixed_precision_speedup, 1)\n", "        })\n", "\n", "    if not valid:\n", "        rt_lat = float(timing_results.get('single_sample_ms', timing_results.get('mean_ms', 0)))\n", "        base_tp = float(timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0)))\n", "        scenarios['real_time_diagnosis'].update({\n", "            'optimal_batch_size': 1,\n", "            'current_latency_ms': round(rt_lat, 2) if rt_lat else None,\n", "            'mixed_precision_latency_ms': round(rt_lat / mixed_precision_speedup, 2) if rt_lat else None\n", "        })\n", "        scenarios['batch_processing'].update({\n", "            'optimal_batch_size': timing_results.get('batch_size', 1),\n", "            'current_throughput_samples_sec': round(base_tp, 1) if base_tp else None,\n", "            'mixed_precision_throughput_samples_sec': round(base_tp * mixed_precision_speedup, 1) if base_tp else None\n", "        })\n", "\n", "    return batch_results, scenarios\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run the complete deployment optimization analysis with all 2 implemented analysis functions above\n", "def run_deployment_optimization_analysis(baseline_model, timing_results, memory_results, detailed_results):\n", "    \"\"\"\n", "    Main function to run all deployment optimization analyses.\n", "    \"\"\"\n", "    print(\"Running Deployment Optimization Analysis...\")\n", "\n", "    # Analyze mixed precision potential\n", "    mixed_precision = analyze_mixed_precision_potential(detailed_results)\n", "    mixed_precision_speedup = mixed_precision['estimated_speedup']\n", "    \n", "    # Analyze batch processing scenarios\n", "    batch_results, batch_scenarios = analyze_batch_processing_scenarios(baseline_model, mixed_precision_speedup)\n", "\n", "    # Visualize batch size analysis for deployment understanding\n", "    plot_batch_size_comparison(batch_results)\n", "\n", "    # Calculate deployment readiness\n", "    current_latency = timing_results['single_sample_ms']\n", "    current_throughput = timing_results['throughput_samples_per_sec']\n", "    \n", "    performance_metrics = {\n", "        'latency_ms': current_latency,\n", "        'throughput_samples_sec': current_throughput\n", "    }\n", "    \n", "    return {\n", "        'mixed_precision': mixed_precision,\n", "        'batch_scenarios': batch_scenarios\n", "    }\n", "\n", "# Execute deployment analysis\n", "deployment_analysis = run_deployment_optimization_analysis(baseline_model, timing_results, memory_results, detailed_results)\n", "\n", "print(f\"\\nDEPLOYMENT OPTIMIZATION RESULTS:\")\n", "mp_details = deployment_analysis['mixed_precision']\n", "print(f\"\\n   Mixed Precision (FP16):\")\n", "print(f\"     Tensor Core Eligible: {mp_details['mixed_precision_coverage_percent']:.1f}%\")\n", "print(f\"     Estimated FLOP improvements: {mp_details['avg_flop_reduction_percent']:.1f}MB\")\n", "print(f\"     Estimated Speedup: {mp_details['estimated_speedup']:.1f}x\")\n", "print(f\"     Estimated Throughput improvements %: {mp_details['throughput_improvement_percent']:.1f}%\")\n", "print(f\"     Estimated Memory Savings: {mp_details['estimated_memory_reduction_mb']:.1f}MB\")\n", "print(f\"     Estimated Sensitivity Risk: {mp_details['sensitivity_risk']}\")\n", "\n", "if 'error' not in deployment_analysis['batch_scenarios']:\n", "    scenarios = deployment_analysis['batch_scenarios']\n", "    print(f\"\\n   Deployment Scenarios:\")\n", "    for scenario, details in scenarios.items():\n", "        print(f\"     {scenario.replace('_', ' ').title()}:\")\n", "        print(f\"       Optimal Batch Size: {details['optimal_batch_size']}\")\n", "        print(f\"       Use Case: {details['use_case']}\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["> **_Before you move on...brainstorming time!_**\n", "> \n", "> Based on your deployment analysis results above, collect your thoughts about hardware deployment opportunities - this will help you in completing your optimization plan at the end of the notebook:\n", ">  \n", "> 1. **Shared-Resource Constraints**: What is the main limiting factor when deploying the model alongside other applications\u2014memory or compute?\n", "> \n", "> 2. **Batch processing trade-offs**: How does performance change with batch size? What's the optimal configuration for different deployment scenarios?\n", "> \n", "> 3. **Mixed precision impact**: What percentage of operations can benefit from FP16? How much speedup can you realistically expect? What are the implementation risks?\n", "> \n", "> 4. **Production Readiness Assessment**: Which KPI targets can be met with hardware acceleration alone?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute projected impact vs UdaciMed targets\n", "import numpy as np\n", "\n", "# Baseline metrics from profiling\n", "baseline_latency_ms = float(timing_results.get('single_sample_ms', timing_results.get('mean_ms', 0)))\n", "baseline_throughput = float(timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0)))\n", "param_mb = float(model_info.get('model_size_mb', 0)) if 'model_info' in globals() else 0.0\n", "\n", "# Pick primary techniques for the plan (avoid double-counting grouped and depthwise; choose depthwise)\n", "interp = arch_analysis['interpolation_removal']['estimated_speedup']\n", "sep = arch_analysis['depthwise_separable']['estimated_speedup']\n", "chan = arch_analysis['channel_organization']['estimated_speedup']\n", "mp = deployment_analysis['mixed_precision']['estimated_speedup']\n", "# Conservative runtime export factor (ONNX/TensorRT/EPs)\n", "onnx_factor = 1.2\n", "combined_speedup = float(interp * sep * chan * mp * onnx_factor)\n", "\n", "proj_latency_ms = round(baseline_latency_ms / combined_speedup, 3) if baseline_latency_ms else None\n", "proj_throughput = round(baseline_throughput * combined_speedup, 1) if baseline_throughput else None\n", "\n", "# Parameter memory projection\n", "sep_param_reduc = arch_analysis['depthwise_separable']['avg_param_reduction_percent'] / 100.0\n", "new_param_mb = param_mb * (1 - sep_param_reduc)\n", "new_param_mb_fp16 = new_param_mb * 0.5  # FP16\n", "\n", "print('Projected combined speedup (interp \u00d7 separable \u00d7 channels_last \u00d7 FP16 \u00d7 ONNX):', f\"{combined_speedup:.2f}x\")\n", "print('Baseline latency (ms):', baseline_latency_ms, '\u2192 Projected latency (ms):', proj_latency_ms)\n", "print('Baseline throughput (sps):', baseline_throughput, '\u2192 Projected throughput (sps):', proj_throughput)\n", "print('Model params memory (MB): baseline', round(param_mb,2), '\u2192 separable', round(new_param_mb,2), '\u2192 +FP16', round(new_param_mb_fp16,2))\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Render final optimization strategy with quantitative justification\n", "from IPython.display import display, Markdown\n", "\n", "interp = arch_analysis['interpolation_removal']\n", "sep = arch_analysis['depthwise_separable']\n", "chan = arch_analysis['channel_organization']\n", "mp = deployment_analysis['mixed_precision']\n", "\n", "onnx_factor = 1.2\n", "combined_speedup = interp['estimated_speedup'] * sep['estimated_speedup'] * chan['estimated_speedup'] * mp['estimated_speedup'] * onnx_factor\n", "baseline_latency_ms = float(timing_results.get('single_sample_ms', timing_results.get('mean_ms', 0)))\n", "baseline_throughput = float(timing_results.get('batch_throughput_samples_per_sec', timing_results.get('throughput_samples_per_sec', 0)))\n", "proj_latency_ms = baseline_latency_ms / combined_speedup if baseline_latency_ms else None\n", "proj_throughput = baseline_throughput * combined_speedup if baseline_throughput else None\n", "param_mb = float(model_info.get('model_size_mb', 0)) if 'model_info' in globals() else 0.0\n", "new_param_mb = param_mb * (1 - sep['avg_param_reduction_percent']/100.0)\n", "new_param_mb_fp16 = new_param_mb * 0.5\n", "\n", "md = f\"\"\"\n", "### Optimization Strategy and Justification (Final Report)\n", "\n", "UdaciMed targets: <100 MB model footprint, >2,000 samples/sec throughput, >98% sensitivity, <3 ms single\u2011sample latency.\n", "\n", "Prioritized plan and rationale:\n", "\n", "1) Remove 64\u2192224 interpolation and adopt a lighter 3\u00d73 stem at native resolution (64\u2013128px)\n", "- Why: Interpolation inflates pixels by ~12.25\u00d7 and dominates early-layer compute/activations.\n", "- Evidence: Interpolation removal estimated speedup ~{interp['estimated_speedup']:.2f}\u00d7; FLOP reduction ~{interp['avg_flop_reduction_percent']:.0f}%.\n", "- Safety: Retrain/fine\u2011tune at native resolution and re\u2011calibrate threshold to maintain \u226598% sensitivity.\n", "\n", "2) Replace 3\u00d73 convolutions with depthwise\u2011separable blocks where channel sizes permit\n", "- Why: 3\u00d73 convs account for most FLOPs; separable cuts k^2\u00b7in\u00b7out to k^2\u00b7in + in\u00b7out.\n", "- Evidence: Candidates: {sep['candidate_layers']}; average param/FLOP reduction ~{sep['avg_flop_reduction_percent']:.0f}%; estimated speedup ~{sep['estimated_speedup']:.2f}\u00d7.\n", "- Memory: Params drop from ~{param_mb:.1f} MB \u2192 ~{new_param_mb:.1f} MB; with FP16 \u2192 ~{new_param_mb_fp16:.1f} MB.\n", "\n", "3) Enable channels_last + in\u2011place activations\n", "- Why: Better memory locality; small but free speedup on most backends.\n", "- Evidence: Estimated speedup ~{chan['estimated_speedup']:.2f}\u00d7.\n", "\n", "4) Mixed precision (FP16) on Tensor\u2011Core GPUs\n", "- Why: Convs/MatMuls dominate compute.\n", "- Evidence: Eligible ops ~{mp['mixed_precision_coverage_percent']:.1f}%; estimated speedup ~{mp['estimated_speedup']:.1f}\u00d7; memory ~50% lower.\n", "\n", "5) Export to ONNX and run with ONNX Runtime/TensorRT EPs\n", "- Why: Kernel fusion and optimized kernels; conservative extra ~1.2\u00d7 speedup.\n", "\n", "6) Batch sizing by scenario\n", "- Real\u2011time: use batch=1\u20138 (choose min per\u2011sample latency);\n", "- Throughput: increase to 16\u2013128 until VRAM plateau; use FP16.\n", "\n", "Projected impact from this run:\n", "- Combined speedup \u2248 {combined_speedup:.2f}\u00d7 \u2192 latency \u2248 {proj_latency_ms:.2f} ms (target <3 ms) and throughput \u2248 {proj_throughput:.0f} sps (will scale further on GPU and larger batches).\n", "- Model memory: params \u2248 {new_param_mb:.1f} MB (separable) \u2192 \u2248 {new_param_mb_fp16:.1f} MB (FP16) \u2014 comfortably <100 MB.\n", "\n", "Implementation sequence:\n", "A) Eliminate interpolation + lighter stem \u2192 B) Depthwise\u2011separable conversion of 3\u00d73 blocks \u2192 C) channels_last + in\u2011place \u2192 D) FP16 training/inference \u2192 E) ONNX export + TensorRT/ORT EP \u2192 F) Batch strategy + threshold re\u2011calibration to keep \u226598% sensitivity.\n", "\n", "Risk/validation:\n", "- Track AUC, sensitivity, specificity after each stage; if sensitivity <98%, raise threshold and/or restore capacity (wider 1\u00d71 pointwise).\n", "- Verify numerics for FP16 on edge cases; fall back to FP32 for clinically sensitive thresholds if needed.\n", "\"\"\"\n", "\n", "display(Markdown(md))\n", "\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## **Congratulations!** \n", "\n", "You have completed the model baseline analysis! This foundational work will guide all subsequent optimization efforts.\n", "\n", "### **Summary: Key findings**\n", "Document your analysis results using this framework:\n", "\n", "1.  **Overall performance profile**: \n", "\n", "- Baseline latency: 36.9 ms single-sample (p95: 50.6 ms; std: 6.9 ms).\n", "- Baseline throughput: 40.7 samples/sec for the profiled batch (single-sample path: 27.1 sps).\n", "- Computational load: ~58.2 GFLOPs for the profiled input; PyTorch profiler shows ~88% of time in convolution ops.\n", "- Memory: peak process memory ~1.56 GB; parameters ~42.6 MB, input ~1.5 MB \u2192 activations dominate.\n", "- Architecture note: inputs are 64\u00d764 but are upsampled to 224\u00d7224 in the model forward() to match ImageNet pretraining \u2192 a 12.25\u00d7 pixel increase (224^2/64^2) that propagates through early layers.\n", "\n", "- Targets (README): <100 MB memory, >2,000 sps throughput, >98% sensitivity, <3 ms latency.\n", "- Current baseline meets sensitivity but misses memory, throughput and latency by large margins (\u22481.56 GB, 40.7 sps, 36.9 ms).\n", "- With architectural changes (eliminate 64\u2192224 upsampling; depthwise/Grouped convs; lighter stem) + mixed precision and deployment acceleration (ONNX/TensorRT), the gaps are addressable in later phases.\n", "\n", "2. **Bottlenecks**: \n", "\n", "- Compute bottleneck: Convolutions dominate (~88% of profiled time); 3\u00d73 convs in residual blocks at higher resolutions are primary contributors.\n", "- Memory bottleneck: Peak ~1.56 GB is driven by activations in early layers where spatial maps are largest; parameters are only ~43 MB.\n", "- Latency bottleneck: Single-sample path \u224837 ms; variance (p95\u224850.6 ms) suggests kernel launch/CPU scheduling overhead on CPU.\n", "- Did you notice? 64\u00d764 inputs are interpolated to 224\u00d7224 inside forward(), inflating both compute and activation memory by ~12.25\u00d7 in the stem and first blocks. Removing this is the single biggest win.\n", "\n", "3. **Architecture optimization**:\n", "\n", "**Checkpoint 1 \u2013 Architecture**<br>\n", "- Candidates for grouped/depthwise separable convolutions: the 3\u00d73 convs in BasicBlocks across layers 2\u20134; they account for most FLOPs while maintaining channel sizes divisible by typical groups.\n", "- First 7\u00d77 stem conv at 224\u00d7224 is expensive; replacing with 3\u00d73 stack (or a stride-2 3\u00d73) reduces FLOPs and improves cache locality.\n", "- Linear classifier (512\u21922) is negligible for both parameters and compute \u2013 not a bottleneck.\n", "- Channel pruning/low\u2011rank factorization can target late-stage 3\u00d73 convs with high redundancy while guarding sensitivity.\n", "_- Top 2 architectural techniques with highest impact potential_<br>\n", "_- Implementation difficulty vs expected benefit analysis_<br>\n", "_- Estimated parameter reduction and optimization goals projections_<br>\n", "_- Other techniques you may consider beyond those listed>>_\n", "\n", "4. **Hardware deployment optimization**: \n", "\n", "**Checkpoint 2 \u2013 Deployment**<br>\n", "- Mixed precision (FP16) is highly applicable: workload is dominated by conv/GEMM; expect ~1.5\u20132.0\u00d7 speedup and ~50% activation/parameter memory reduction on Tensor Core GPUs.\n", "- Batch\u2011size strategy: for real\u2011time, use batch=1 (focus on median and p95); for throughput, increase to 8\u201332 until latency/VRAM plateau.\n", "- Runtime: export to ONNX and run with ONNX Runtime/TensorRT EP to fuse ops and maximize GPU utilization.\n", "_- Mixed precision acceleration potential and implementation plan_<br>\n", "_- Optimal batch configurations for different use cases_>>\n", "\n", "### **Recommended optimization roadmap**\n", "\n", "Based on the analysis, prioritize the optimization techniques and highlight the estimated combined impact on optimization goals for each phase:\n", "\n", "**Phase 1 (Quick Wins):**\n", "\n", "- Remove 64\u2192224 interpolation; train/finetune at native 64\u2013128 input with an adjusted stem (3\u00d73).\n", "- Replace heavy 3\u00d73 convs with depthwise\u2011separable (depthwise + 1\u00d71 pointwise) or grouped convs where channels permit.\n", "- Consider inverted residual blocks (MobileNetV2\u2011style) in high\u2011resolution stages.\n", "\n", "- Enable FP16 inference; calibrate threshold to preserve >98% sensitivity.\n", "- Export to ONNX and accelerate with TensorRT/ONNX Runtime EP; use dynamic batch for screening pipelines.\n", "\n", "- Order\u2011of\u2011magnitude impact estimate (on T4\u2011class GPU):\n", "  \u2022 Remove upsampling: up to ~12\u00d7 less compute/activations in early blocks.\n", "  \u2022 Depthwise/grouped convs on 3\u00d73 layers: ~3\u20138\u00d7 layer\u2011wise FLOP reduction.\n", "  \u2022 FP16 + TensorRT: additional ~1.5\u20132.5\u00d7. Combined, a >20\u00d7 speed/throughput gain is realistic while cutting memory to <100 MB.\n", "\n", "**Phase 2 (Extra Impact):**\n", "\n", "- Quantization\u2011aware training to INT8 (post\u2011validation) for edge targets.\n", "- Fuse BatchNorm into conv at export; prefer static shapes for better kernel selection.\n", "- Profile multiple batch sizes to locate the knee point for your hardware.\n", "\n", "- Deployment\u2011side optimizations above typically add another 1.2\u20132.0\u00d7 beyond architecture changes and enable scaling to high\u2011volume screening workloads.\n", "\n", "---\n", "\n", "**You are now ready to move to Notebook 2: Architecture Optimization!**"]}], "metadata": {"kernelspec": {"display_name": "base", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]"}, "vscode": {"interpreter": {"hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}}, "nbformat": 4, "nbformat_minor": 4}