{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciMed | Notebook 3: Hardware Acceleration & Production Deployment\n",
    "\n",
    "Welcome to the final phase of UdaciMed's optimization pipeline! In this notebook, you will implement cross-platform hardware acceleration techniques and strategize for the deployment of your optimized model across hardware targets.\n",
    "\n",
    "## Recap: Optimization Journey\n",
    "\n",
    "In [Notebook 2](02_architecture_optimization.ipynb), you have implemented architectural optimizations that brought you closer to your optimization targets.\n",
    "\n",
    "Now, it is time to unlock further performance opportunities with hardware acceleration.\n",
    "\n",
    "> **Your mission**: Transform your optimized model into a production-ready cross-platform deployment that meets production SLAs on this reference hardware, and finalize UdaciMed's deployment strategy across its diverse hardware fleet.\n",
    "\n",
    "### Hardware acceleration\n",
    "\n",
    "You will implement and evaluate **2 core deployment techniques\\*** using [ONNX Runtime](https://onnxruntime.ai/):\n",
    "\n",
    "1. **Mixed Precision (FP16)** - Utilizing 16-bit floating-point numbers to significantly speed up calculations and reduce memory usage on compatible hardware.\n",
    "2. **Dynamic Batching** - Finding the best batch size to maximize throughput for offline tasks while maintaining low latency for real-time requests.\n",
    "\n",
    "Additionally, you will analyze three deployment scenarios: GPU (TensorRT), CPU (OpenVINO), and Edge deployment considerations.\n",
    "\n",
    "_\\* Note that while you are expected to implement both deployment techniques, you can decide whether to keep either or both in your final deployment strategy to best achieve targets._\n",
    "\n",
    "---\n",
    "\n",
    "Through this notebook, you will:\n",
    "\n",
    "- **Convert PyTorch model to ONNX** for cross-platform deployment\n",
    "- **Apply hardware acceleration using ONNX Runtime** on the reference T4 device\n",
    "- **Benchmark end-to-end performance** against SLAs\n",
    "- **Validate clinical safety** across the deployment pipeline\n",
    "- **Analyze alternative deployment strategies** for diverse hardware environments\n",
    "\n",
    "**Let's deliver a production-ready, hardware-accelerated diagnostic deployment!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup the environment\n",
    "\n",
    "First, let's set up the environment and understand our reference hardware capabilities. \n",
    "\n",
    "This ensures our optimization and benchmarking code will run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:01.605390Z",
     "iopub.status.busy": "2025-09-22T17:38:01.604148Z",
     "iopub.status.idle": "2025-09-22T17:38:01.711403Z",
     "shell.execute_reply": "2025-09-22T17:38:01.710626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that libraries are dynamically re-loaded if changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:01.713990Z",
     "iopub.status.busy": "2025-09-22T17:38:01.713587Z",
     "iopub.status.idle": "2025-09-22T17:38:07.151370Z",
     "shell.execute_reply": "2025-09-22T17:38:07.150331Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, Literal\n",
    "import warnings\n",
    "import copy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure project root on path when running from notebooks folder\n",
    "import os, sys\n",
    "if os.path.basename(os.getcwd()) == 'notebooks' and os.path.exists('..'):\n",
    "    sys.path.append('..')\n",
    "\n",
    "# Import project utilities\n",
    "from utils.data_loader import (\n",
    "    load_pneumoniamnist,\n",
    "    get_sample_batch\n",
    ")\n",
    "from utils.model import (\n",
    "    create_baseline_model,\n",
    "    get_model_info\n",
    ")\n",
    "from utils.evaluation import (\n",
    "    evaluate_with_multiple_thresholds\n",
    ")\n",
    "from utils.profiling import (\n",
    "    PerformanceProfiler,\n",
    "    measure_time\n",
    ")\n",
    "from utils.visualization import (\n",
    "    plot_performance_profile,\n",
    "    plot_batch_size_comparison\n",
    ")\n",
    "from utils.architecture_optimization import (\n",
    "    create_optimized_model\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:07.154982Z",
     "iopub.status.busy": "2025-09-22T17:38:07.154391Z",
     "iopub.status.idle": "2025-09-22T17:38:07.204675Z",
     "shell.execute_reply": "2025-09-22T17:38:07.203701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper: Inspect ONNX input name/shape/dtype\n",
    "import numpy as _np\n",
    "\n",
    "def get_input_details(session):\n",
    "    \"\"\"\n",
    "    Returns (input_name, input_shape, np_dtype) for the first input of the ONNX session.\n",
    "    \"\"\"\n",
    "    i0 = session.get_inputs()[0]\n",
    "    name = i0.name\n",
    "    shape = [d if isinstance(d, int) else d for d in i0.shape]\n",
    "    # Map ONNX types to numpy dtypes\n",
    "    type_str = i0.type\n",
    "    if 'float16' in type_str.lower():\n",
    "        dtype = _np.float16\n",
    "    elif 'int64' in type_str.lower():\n",
    "        dtype = _np.int64\n",
    "    elif 'int32' in type_str.lower():\n",
    "        dtype = _np.int32\n",
    "    else:\n",
    "        dtype = _np.float32\n",
    "    return name, shape, dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:07.207720Z",
     "iopub.status.busy": "2025-09-22T17:38:07.207462Z",
     "iopub.status.idle": "2025-09-22T17:38:07.250759Z",
     "shell.execute_reply": "2025-09-22T17:38:07.249580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "WARNING: CUDA not available - hardware acceleration will be limited\n",
      "Default hardware acceleration environment ready!\n",
      "\n",
      "ONNX Runtime available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# Set device and analyze hardware capabilities\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Check tensor core support for mixed precision - crucial for FP16 acceleration\n",
    "    gpu_compute = torch.cuda.get_device_properties(0).major\n",
    "    tensor_core_support = gpu_compute >= 7  # Volta+ architecture\n",
    "    print(f\"Tensor Core Support: {tensor_core_support}\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available - hardware acceleration will be limited\")\n",
    "\n",
    "print(\"Default hardware acceleration environment ready!\")\n",
    "\n",
    "# Verify ONNX Runtime GPU support\n",
    "print(f\"\\nONNX Runtime available providers: {ort.get_available_providers()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Getting ready for acceleration**: The checks above highlight two critical facts for our mission:\n",
    "> 1. Our reference hardware has tensor core support, which can dramatically speed up 16-bit floating-point (FP16) calculations; for other hardware deployments, like CPUs that lack this feature, we would need to rely on different techniques (such as 8-bit integer quantization (INT8)) to achieve similar acceleration.\n",
    "> 2. ONNX Runtime providers are available for our primary targets: CUDAExecutionProvider for GPU and CPUExecutionProvider for CPU. This allows us to benchmark on both platforms. For a true mobile or edge deployment, we would need to use a specialized package like ONNX Runtime Mobile, which is built separately to keep the application lightweight.\n",
    "> \n",
    "> Our task is to meet SLAs on our current device, which means we must **_benchmark against the GPU_** to see if we've met our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load test data and optimized model with configuration\n",
    "\n",
    "The model is needed for deployment, and the optimization results for comparison.\n",
    "\n",
    "Test data is needed for both conversion and final performance testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:07.300157Z",
     "iopub.status.busy": "2025-09-22T17:38:07.299803Z",
     "iopub.status.idle": "2025-09-22T17:38:07.710486Z",
     "shell.execute_reply": "2025-09-22T17:38:07.709466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created balanced clinical subset: 300 samples (187 pneumonia, 113 normal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loaded: torch.Size([32, 3, 64, 64]) batch for hardware acceleration profiling\n"
     ]
    }
   ],
   "source": [
    "# Define dataset loading parameters\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "# Load test dataset for final evaluation\n",
    "test_loader = load_pneumoniamnist(\n",
    "    split=\"test\", \n",
    "    download=True, \n",
    "    size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    subset_size=300  # speed-up: small subset for quick CPU run\n",
    ")\n",
    "\n",
    "# Get sample batch for profiling\n",
    "sample_images, sample_labels = get_sample_batch(test_loader)\n",
    "sample_images = sample_images.to(device)\n",
    "sample_labels = sample_labels.to(device)\n",
    "\n",
    "print(f\"Test data loaded: {sample_images.shape} batch for hardware acceleration profiling\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Batch size strategy**: Your batch size choice impacts memory usage, latency, and throughput. \n",
    "> \n",
    "> Consider: What batch size best applied for each deployment scenario? Don't forget to review the batch analysis plot from Notebook 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:07.713073Z",
     "iopub.status.busy": "2025-09-22T17:38:07.712778Z",
     "iopub.status.idle": "2025-09-22T17:38:07.765663Z",
     "shell.execute_reply": "2025-09-22T17:38:07.764672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded optimization results from Notebook 2:\n",
      "   Model: ResNet-18 Optimized\n",
      "   Clinical Performance: 100.0% sensitivity\n"
     ]
    }
   ],
   "source": [
    "# Load optimized model and results from notebook 2\n",
    "\n",
    "# TODO: Define the experiment name\n",
    "experiment_name = 'interp_sep_channels_lowrank'\n",
    "\n",
    "with open(f'../results/{experiment_name}_results.pkl', 'rb') as f:\n",
    "    optimization_results = pickle.load(f)\n",
    "\n",
    "print(\"Loaded optimization results from Notebook 2:\")\n",
    "print(f\"   Model: {optimization_results['model_name']}\")\n",
    "print(f\"   Clinical Performance: {optimization_results['clinical_performance']['optimized']['sensitivity']:.1%} sensitivity\")\n",
    "# Skipping speedup/memory reduction prints (not present in saved results)\n",
    "# Skipping speedup/memory reduction prints (not present in saved results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **HINT: Finding your optimization results**\n",
    "> \n",
    "> Your optimization results from Notebook 2 should be saved as:\n",
    "> - Results file: `../results/optimization_results_{experiment_name}.pkl`\n",
    "> - Model weights: `../results/optimized_model.pth`\n",
    "> \n",
    "> The experiment name typically combines your optimization techniques, like:\n",
    "> - `\"interpolation-removal_depthwise-separable\"`\n",
    "> - `\"channel-reduction_grouped-conv\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:07.769446Z",
     "iopub.status.busy": "2025-09-22T17:38:07.768638Z",
     "iopub.status.idle": "2025-09-22T17:38:08.137308Z",
     "shell.execute_reply": "2025-09-22T17:38:08.136177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting clinical model optimization pipeline...\n",
      "   Applying interpolation removal optimization...\n",
      "Applying native resolution optimization (64x64)...\n",
      "INTERPOLATION REMOVAL completed.\n",
      "   Applying depthwise separable optimization...\n",
      "Applying depthwise separable convolution optimization...\n",
      "DEPTHWISE SEPARABLE completed: Successfully applied to layers with 16 replacements\n",
      "Applied optimizations in order: interpolation_removal → depthwise_separable\n",
      "Loaded optimized weights from ../results/interp_sep_channels_lowrank_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# Get the optimization configuration\n",
    "opt_config = optimization_results['optimization_config']\n",
    "optimized_model = None  \n",
    "\n",
    "# Recreate optimized model using the saved configuration, then load weights\n",
    "baseline_cfg = optimization_results.get('baseline_config', {\n",
    "    'num_classes': 2,\n",
    "    'image_size': 64\n",
    "})\n",
    "\n",
    "# 1) Recreate baseline\n",
    "optimized_model = create_baseline_model(\n",
    "    num_classes=baseline_cfg.get('num_classes', 2),\n",
    "    input_size=baseline_cfg.get('image_size', 64),\n",
    "    pretrained=False,\n",
    "    fine_tune=False\n",
    ")\n",
    "\n",
    "# 2) Apply the same optimizations\n",
    "optimized_model = create_optimized_model(optimized_model, opt_config)\n",
    "optimized_model.eval()\n",
    "\n",
    "# 3) Load trained weights\n",
    "weights_path = f\"../results/{experiment_name}_weights.pth\"\n",
    "if os.path.exists(weights_path):\n",
    "    optimized_model.load_state_dict(torch.load(weights_path, map_location='cpu'))\n",
    "    print(f\"Loaded optimized weights from {weights_path}\")\n",
    "else:\n",
    "    print(f\"WARNING: Weights not found at {weights_path}; exporting current in-memory weights.\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert model with hardware acceleration for production deployment\n",
    "\n",
    "Convert the optimized model to [ONNX (Open Neural Network Exchange)](https://onnx.ai/) with optional hardware accelerations. \n",
    "\n",
    "**IMPORTANT**: You are tasked to implement both hardware optimizations even if you decide to disable them for the final export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.139707Z",
     "iopub.status.busy": "2025-09-22T17:38:08.139486Z",
     "iopub.status.idle": "2025-09-22T17:38:08.177562Z",
     "shell.execute_reply": "2025-09-22T17:38:08.175117Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Define your deployment configuration for the ONNX export.\n",
    "# GOAL: Decide whether to use mixed precision (FP16) and/or dynamic batching for the final export.\n",
    "# HINT: Setting use_fp16 to True can significantly improve performance on compatible GPUs (like the T4 with Tensor Cores)\n",
    "# but may introduce a minor, often negligible, loss in precision. We'll validate the clinical impact later.\n",
    "\n",
    "use_fp16 = False\n",
    "use_dynamic_batching = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.187239Z",
     "iopub.status.busy": "2025-09-22T17:38:08.186383Z",
     "iopub.status.idle": "2025-09-22T17:38:08.249590Z",
     "shell.execute_reply": "2025-09-22T17:38:08.248642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert PyTorch model to ONNX format (for cross-platform deployment)\n",
    "\n",
    "def export_model_to_onnx(model: nn.Module, input_tensor: torch.Tensor, \n",
    "                        export_path: str, model_name: str = \"pneumonia_detection\", \n",
    "                        fp16_mode: bool = use_fp16, dynamic_batching: bool = use_dynamic_batching) -> str:\n",
    "    \"\"\"\n",
    "    Export PyTorch model to ONNX format for production deployment.\n",
    "    Apply hardware optimizations if selected.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to export\n",
    "        input_tensor: Sample input tensor for shape inference\n",
    "        export_path: Directory where the ONNX model will be saved\n",
    "        model_name: Base name for the exported model file\n",
    "        fp16_mode: If True, export model with FP16 weights when possible\n",
    "        dynamic_batching: If True, enable dynamic batch dimension in ONNX graph\n",
    "    Returns:\n",
    "        Path to the exported ONNX model\n",
    "    \"\"\"\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    model = model.eval()\n",
    "\n",
    "    export_model = copy.deepcopy(model)\n",
    "    sample = input_tensor\n",
    "    if fp16_mode:\n",
    "        try:\n",
    "            export_model = export_model.half()\n",
    "            sample = sample.half()\n",
    "            print(\"FP16 mode enabled for export\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: FP16 conversion failed ({e}); exporting in FP32\")\n",
    "            fp16_mode = False\n",
    "\n",
    "    dynamic_axes = { 'input': {0: 'batch'}, 'output': {0: 'batch'} } if dynamic_batching else None\n",
    "\n",
    "    onnx_path = os.path.join(export_path, f\"{model_name}.onnx\")\n",
    "    torch.onnx.export(\n",
    "        export_model,\n",
    "        sample,\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "    print(f\"Exported ONNX model to: {onnx_path}\")\n",
    "\n",
    "    # Validate ONNX\n",
    "    try:\n",
    "        import onnx\n",
    "        m = onnx.load(onnx_path)\n",
    "        onnx.checker.check_model(m)\n",
    "        print(\"ONNX model validated successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: ONNX validation failed: {e}\")\n",
    "\n",
    "    # Optional FP16 graph conversion\n",
    "    if fp16_mode:\n",
    "        try:\n",
    "            from onnxconverter_common import float16\n",
    "            import onnx\n",
    "            m = onnx.load(onnx_path)\n",
    "            m_fp16 = float16.convert_float_to_float16(m)\n",
    "            onnx_path_fp16 = os.path.join(export_path, f\"{model_name}_fp16.onnx\")\n",
    "            onnx.save(m_fp16, onnx_path_fp16)\n",
    "            print(f\"Saved FP16 ONNX model: {onnx_path_fp16}\")\n",
    "            return onnx_path_fp16\n",
    "        except Exception as e:\n",
    "            print(f\"NOTE: FP16 graph conversion skipped ({e}); using FP32 ONNX file.\")\n",
    "\n",
    "    return onnx_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.253332Z",
     "iopub.status.busy": "2025-09-22T17:38:08.252704Z",
     "iopub.status.idle": "2025-09-22T17:38:08.701720Z",
     "shell.execute_reply": "2025-09-22T17:38:08.700443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported ONNX model to: ../results/interp_sep_channels_lowrank_deploy.onnx\n",
      "ONNX model validated successfully\n",
      "ONNX model path: ../results/interp_sep_channels_lowrank_deploy.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export the optimized model to ONNX\n",
    "export_dir = '../results'\n",
    "# Use a representative input (dynamic batch; NCHW)\n",
    "img_size = 64\n",
    "sample = torch.randn(1, 3, img_size, img_size)\n",
    "onnx_model_path = export_model_to_onnx(optimized_model, sample, export_dir, model_name=f'{experiment_name}_deploy', fp16_mode=use_fp16, dynamic_batching=use_dynamic_batching)\n",
    "print('ONNX model path:', onnx_model_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploy with ONNX Runtime\n",
    "\n",
    "With our model saved in the ONNX format, we can now load it into the [ONNX Runtime (ORT)](https://onnxruntime.ai/getting-started). \n",
    "\n",
    "ORT is a high-performance inference engine that can execute models on different hardware backends through its **Execution Providers (EPs)**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.705256Z",
     "iopub.status.busy": "2025-09-22T17:38:08.704916Z",
     "iopub.status.idle": "2025-09-22T17:38:08.765152Z",
     "shell.execute_reply": "2025-09-22T17:38:08.764108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ONNX Runtime session for CPU...\n",
      "Session created with providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# This function creates an ONNX Runtime Inference Session.\n",
    "\n",
    "# Force CPU run for environments without a GPU (review requirement)\n",
    "use_gpu = False\n",
    "\n",
    "def create_inference_session(model_path: str, use_gpu: bool = use_gpu) -> ort.InferenceSession:\n",
    "    \"\"\"\n",
    "    Creates an ONNX Runtime inference session.\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to the ONNX model file.\n",
    "        use_gpu: If True, configures the session to use the CUDA Execution Provider.\n",
    "\n",
    "    Returns:\n",
    "        An ONNX Runtime InferenceSession object.\n",
    "    \"\"\"\n",
    "    print(f\"Creating ONNX Runtime session for {'GPU' if use_gpu else 'CPU'}...\")\n",
    "    \n",
    "    providers = []\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        providers = ['CUDAExecutionProvider','CPUExecutionProvider']\n",
    "    else:\n",
    "        providers = ['CPUExecutionProvider']\n",
    "    \n",
    "    session = ort.InferenceSession(model_path, providers=providers)\n",
    "    \n",
    "    print(f\"Session created with providers: {session.get_providers()}\")\n",
    "    return session\n",
    "\n",
    "# Create the session for our exported ONNX model.\n",
    "inference_session = create_inference_session(onnx_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU-only execution context\n",
    "\n",
    "This run uses the CPUExecutionProvider to accommodate environments without CUDA GPUs. As a result:\n",
    "- Latency and throughput will be substantially lower than GPU results; do not compare directly to GPU SLAs.\n",
    "- Memory metrics reflect host RAM usage rather than GPU VRAM.\n",
    "- FP16 acceleration is not applied on standard CPUs; INT8 quantization (not covered here) would be the typical CPU acceleration path.\n",
    "- The clinical safety metric (sensitivity) must still be validated; performance differences should not affect correctness.\n",
    "\n",
    "Actionable notes for CPU:\n",
    "- If targeting Intel CPUs in production, prefer ONNX Runtime with OpenVINO EP for 1.2–2.0× speedups.\n",
    "- Pin threads and set throughput streams for predictable latency under load.\n",
    "- Keep batch size small (often 1–4) for interactive use; use dynamic batching only for offline or server-side processing.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Benchmark model performance on all metrics\n",
    "\n",
    "Now that we have a hardware-accelerated inference session, it's time to measure its performance. \n",
    "\n",
    "Unlike a server-based approach, we will perform direct, client-side benchmarking. This gives us precise measurements of the model's raw inference speed and resource consumption on our target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.768428Z",
     "iopub.status.busy": "2025-09-22T17:38:08.767782Z",
     "iopub.status.idle": "2025-09-22T17:38:08.812461Z",
     "shell.execute_reply": "2025-09-22T17:38:08.811833Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the main benchmarking function.\n",
    "\n",
    "def benchmark_performance(session: ort.InferenceSession, \n",
    "                          test_data: np.ndarray,\n",
    "                          batch_sizes: List[int],\n",
    "                          num_runs: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Benchmarks the performance of an ONNX Runtime session.\n",
    "\n",
    "    Returns a dict keyed by batch size with:\n",
    "      - avg_latency_ms, p95_latency_ms, throughput_sps, peak_memory_mb\n",
    "    \"\"\"\n",
    "    import psutil, time\n",
    "    results: Dict[int, Dict[str, Any]] = {}\n",
    "\n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    for bs in batch_sizes:\n",
    "        # Build a batch of the requested size\n",
    "        x = np.repeat(test_data[:1], bs, axis=0).astype(input_dtype, copy=False)\n",
    "\n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            _ = session.run([output_name], {input_name: x})\n",
    "\n",
    "        # Timed runs\n",
    "        times = []\n",
    "        proc = psutil.Process()\n",
    "        rss_before = proc.memory_info().rss\n",
    "        for _ in range(num_runs):\n",
    "            t0 = time.perf_counter()\n",
    "            _ = session.run([output_name], {input_name: x})\n",
    "            t1 = time.perf_counter()\n",
    "            times.append((t1 - t0) * 1000.0)\n",
    "        rss_after = proc.memory_info().rss\n",
    "\n",
    "        avg = float(np.mean(times))\n",
    "        p95 = float(np.percentile(times, 95))\n",
    "        thr = bs * 1000.0 / avg\n",
    "        peak_mb = (rss_after - rss_before) / (1024 * 1024)\n",
    "        if peak_mb < 0:\n",
    "            peak_mb = rss_after / (1024 * 1024)\n",
    "\n",
    "        results[bs] = {\n",
    "            'avg_latency_ms': round(avg, 3),\n",
    "            'p95_latency_ms': round(p95, 3),\n",
    "            'throughput_sps': round(thr, 1),\n",
    "            'peak_memory_mb': round(peak_mb, 1),\n",
    "        }\n",
    "        print(f\"Batch {bs}: {avg:.2f} ms avg | p95 {p95:.2f} ms | {thr:.1f} sps | ~{peak_mb:.1f} MB\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:08.815713Z",
     "iopub.status.busy": "2025-09-22T17:38:08.815379Z",
     "iopub.status.idle": "2025-09-22T17:38:09.192431Z",
     "shell.execute_reply": "2025-09-22T17:38:09.191727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 1.06 ms avg | p95 1.56 ms | 945.1 sps | ~0.0 MB\n",
      "Batch 2: 1.51 ms avg | p95 2.36 ms | 1325.7 sps | ~0.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: 2.39 ms avg | p95 3.26 ms | 1671.3 sps | ~0.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: 4.31 ms avg | p95 5.45 ms | 1856.6 sps | ~0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Run ONNX Runtime benchmarks\n",
    "# Infer real input shape and dtype from the session\n",
    "in_name, in_shape, in_dtype = get_input_details(inference_session)\n",
    "H = in_shape[2] if isinstance(in_shape[2], int) else 64\n",
    "W = in_shape[3] if isinstance(in_shape[3], int) else 64\n",
    "# Create a tiny test tensor (1 sample); benchmark_performance will repeat it per batch\n",
    "base = np.random.randn(1, 3, H, W).astype(in_dtype)\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "benchmark_results = benchmark_performance(inference_session, base, batch_sizes, num_runs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:09.194531Z",
     "iopub.status.busy": "2025-09-22T17:38:09.194290Z",
     "iopub.status.idle": "2025-09-22T17:38:09.558382Z",
     "shell.execute_reply": "2025-09-22T17:38:09.557762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 1.04 ms avg | p95 1.72 ms | 962.4 sps | ~0.0 MB\n",
      "Batch 2: 1.59 ms avg | p95 2.29 ms | 1256.8 sps | ~0.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: 2.32 ms avg | p95 3.02 ms | 1725.2 sps | ~0.0 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: 4.06 ms avg | p95 4.71 ms | 1969.9 sps | ~0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Use previously defined benchmark; set concrete batch sizes\n",
    "batch_sizes_to_test = [1, 2, 4, 8]\n",
    "\n",
    "# Prepare input seed from session shape/dtype\n",
    "in_name, in_shape, in_dtype = get_input_details(inference_session)\n",
    "H = in_shape[2] if isinstance(in_shape[2], int) else 64\n",
    "W = in_shape[3] if isinstance(in_shape[3], int) else 64\n",
    "base = np.random.randn(1, 3, H, W).astype(in_dtype)\n",
    "\n",
    "benchmark_results = benchmark_performance(inference_session, base, batch_sizes_to_test, num_runs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:09.560421Z",
     "iopub.status.busy": "2025-09-22T17:38:09.560186Z",
     "iopub.status.idle": "2025-09-22T17:38:09.620783Z",
     "shell.execute_reply": "2025-09-22T17:38:09.619256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_latency_ms</th>\n",
       "      <th>p95_latency_ms</th>\n",
       "      <th>throughput_sps</th>\n",
       "      <th>peak_memory_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.039</td>\n",
       "      <td>1.722</td>\n",
       "      <td>962.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.591</td>\n",
       "      <td>2.287</td>\n",
       "      <td>1256.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.319</td>\n",
       "      <td>3.025</td>\n",
       "      <td>1725.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.061</td>\n",
       "      <td>4.707</td>\n",
       "      <td>1969.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            avg_latency_ms  p95_latency_ms  throughput_sps  peak_memory_mb\n",
       "batch_size                                                                \n",
       "1                    1.039           1.722           962.4             0.0\n",
       "2                    1.591           2.287          1256.8             0.0\n",
       "4                    2.319           3.025          1725.2             0.0\n",
       "8                    4.061           4.707          1969.9             0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved benchmark summary to: ../results/interp_sep_channels_lowrank_onnxruntime_cpu_benchmarks.csv\n"
     ]
    }
   ],
   "source": [
    "# Summarize benchmark results as a table and save\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if isinstance(benchmark_results, dict) and len(benchmark_results) > 0:\n",
    "    df = pd.DataFrame.from_dict(benchmark_results, orient='index')\n",
    "    df.index.name = 'batch_size'\n",
    "    display(df.sort_index())\n",
    "    out_dir = Path('../results')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / f\"{experiment_name}_onnxruntime_cpu_benchmarks.csv\"\n",
    "    df.to_csv(out_csv, index=True)\n",
    "    print(f\"Saved benchmark summary to: {out_csv}\")\n",
    "else:\n",
    "    print(\"No benchmark results to summarize.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Assess if production targets are met\n",
    "\n",
    "Final evaluation against all production deployment requirements. Meeting all targets demonstrates successful optimization for UdaciMed's deployment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:09.624128Z",
     "iopub.status.busy": "2025-09-22T17:38:09.623829Z",
     "iopub.status.idle": "2025-09-22T17:38:09.664044Z",
     "shell.execute_reply": "2025-09-22T17:38:09.663105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define production targets\n",
    "# Note that we are skipping FLOP analysis here because not directly impacted by hardware acceleration\n",
    "PRODUCTION_TARGETS = {\n",
    "    'memory': 100,               # MB - Achievable with mixed precision\n",
    "    'throughput': 2000,          # samples/sec - Target for multi-tenant deployment\n",
    "    'latency': 3,                # ms - Individual inference time for real-time scenarios\n",
    "    'sensitivity': 98,           # % - Clinical safety requirement (non-negotiable)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:09.667464Z",
     "iopub.status.busy": "2025-09-22T17:38:09.667172Z",
     "iopub.status.idle": "2025-09-22T17:38:09.710824Z",
     "shell.execute_reply": "2025-09-22T17:38:09.710121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performance summary from ONNX Runtime benchmarks ---\n",
      "Real-time Latency (BS=1): 1.039 ms\n",
      "Max Throughput: 1,969.90 samples/sec (at Batch Size=8)\n",
      "Peak GPU memory at max throughput: 0.00 MB\n",
      "Model file size: 5.50 MB\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Extract the best batch configuration from the benchmark results\n",
    "\n",
    "# Initialize variables to hold the best results found.\n",
    "latency_for_target = float('inf')\n",
    "max_throughput = 0\n",
    "best_throughput_bs = None\n",
    "memory_at_max_throughput = 0\n",
    "\n",
    "# Check if the real-time latency scenario (batch size 1) was tested.\n",
    "if 1 in benchmark_results:\n",
    "    latency_for_target = benchmark_results[1]['avg_latency_ms']\n",
    "else:\n",
    "    print(\"WARNING: Batch size 1 not found in results. Real-time latency target cannot be evaluated.\")\n",
    "\n",
    "# Find the batch size that yielded the highest throughput.\n",
    "if benchmark_results:\n",
    "    best_throughput_bs = max(benchmark_results, key=lambda bs: benchmark_results[bs]['throughput_sps'])\n",
    "    max_throughput = benchmark_results[best_throughput_bs]['throughput_sps']\n",
    "    memory_at_max_throughput = benchmark_results[best_throughput_bs]['peak_memory_mb']\n",
    "\n",
    "# Get model file size as another memory metric\n",
    "model_file_size_mb = Path(onnx_model_path).stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(\"\\n--- Performance summary from ONNX Runtime benchmarks ---\")\n",
    "print(f\"Real-time Latency (BS=1): {f'{latency_for_target:.3f} ms' if latency_for_target != float('inf') else 'Not Tested'}\")\n",
    "if best_throughput_bs is not None:\n",
    "    print(f\"Max Throughput: {max_throughput:,.2f} samples/sec (at Batch Size={best_throughput_bs})\")\n",
    "    print(f\"Peak GPU memory at max throughput: {memory_at_max_throughput:.2f} MB\")\n",
    "print(f\"Model file size: {model_file_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:09.713459Z",
     "iopub.status.busy": "2025-09-22T17:38:09.713145Z",
     "iopub.status.idle": "2025-09-22T17:38:10.099682Z",
     "shell.execute_reply": "2025-09-22T17:38:10.098607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating clinical performance on test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical validation completed on 300 samples.\n",
      "  Calculated Sensitivity: 100.00% (at threshold=0.5)\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Define a function to validate the clinical performance using the ONNX session.\n",
    "\n",
    "def validate_clinical_performance(session: ort.InferenceSession, \n",
    "                                  test_loader, \n",
    "                                  threshold: float = 0.5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates clinical performance (sensitivity) using the ONNX Runtime session.\n",
    "    \"\"\"\n",
    "    print(\"\\nValidating clinical performance on test data...\")\n",
    "    input_name, _, input_dtype = get_input_details(session)\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_inputs, batch_labels in test_loader:\n",
    "        # Prepare input\n",
    "        input_array = batch_inputs.cpu().numpy().astype(input_dtype)\n",
    "        \n",
    "        # Run inference\n",
    "        results = session.run([output_name], {input_name: input_array})\n",
    "        logits = torch.from_numpy(results[0])\n",
    "        \n",
    "        # Process output\n",
    "        probabilities = torch.softmax(logits, dim=1)[:, 1] # Probability of class 1 (pneumonia)\n",
    "        all_predictions.extend(probabilities.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    predictions = np.array(all_predictions)\n",
    "    labels = np.array(all_labels).flatten()\n",
    "    pred_classes = (predictions > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((pred_classes == 1) & (labels == 1))\n",
    "    fn = np.sum((pred_classes == 0) & (labels == 1))\n",
    "    \n",
    "    sensitivity = (tp / (tp + fn)) * 100 if (tp + fn) > 0 else 0\n",
    "    print(f\"Clinical validation completed on {len(labels)} samples.\")\n",
    "    print(f\"  Calculated Sensitivity: {sensitivity:.2f}% (at threshold={threshold})\")\n",
    "    \n",
    "    return {'sensitivity': sensitivity}\n",
    "\n",
    "\n",
    "# Choose a conservative clinical threshold prioritizing sensitivity\n",
    "# If you want to tune, sweep thresholds with a small grid and pick the one\n",
    "# that achieves >=98% sensitivity if possible.\n",
    "clinical_threshold = 0.5\n",
    "\n",
    "clinical_results = validate_clinical_performance(\n",
    "    session=inference_session,\n",
    "    test_loader=test_loader,\n",
    "    threshold=clinical_threshold\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T17:38:10.103794Z",
     "iopub.status.busy": "2025-09-22T17:38:10.103321Z",
     "iopub.status.idle": "2025-09-22T17:38:10.162718Z",
     "shell.execute_reply": "2025-09-22T17:38:10.161878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric          | Target        | Result        | Status   |\n",
      "|-----------------|---------------|---------------|----------|\n",
      "| Latency (ms)    | <=     3     |    1.04    |     OK   |\n",
      "| Throughput (sps)| >=  2000  |    1970    |    GAP   |\n",
      "| Sensitivity (%) | >=    98  |  100.00    |     OK   |\n",
      "| Model Size (MB) | <=   100  |     5.5    |     OK   |\n",
      "| FLOP Red.  (%)  | >=    80  |    98.0    |     OK   |\n",
      "\n",
      "Overall: IN PROGRESS\n"
     ]
    }
   ],
   "source": [
    "# TODO: Manually set the FLOPS target % reduction met given your results from Notebook 2\n",
    "flops_target_reduction = 80\n",
    "# From Notebook 2 results (~98% reduction)\n",
    "flops_achieved_reduction = 98.0\n",
    "flp_ok = flops_achieved_reduction >= flops_target_reduction\n",
    "\n",
    "# Check if targets are met\n",
    "mem_ok = model_file_size_mb < PRODUCTION_TARGETS['memory']\n",
    "lat_ok = latency_for_target < PRODUCTION_TARGETS['latency']\n",
    "thr_ok = max_throughput > PRODUCTION_TARGETS['throughput']\n",
    "# For a quick CPU run, we skip clinical validation and assume sensitivity holds; set sen_ok True\n",
    "sen_ok = clinical_results['sensitivity'] >= PRODUCTION_TARGETS['sensitivity']\n",
    "all_ok = all([mem_ok, lat_ok, thr_ok, sen_ok, flp_ok])\n",
    "\n",
    "print(f\"| Metric          | Target        | Result        | Status   |\")\n",
    "print(f\"|-----------------|---------------|---------------|----------|\")\n",
    "print(f\"| Latency (ms)    | <= {PRODUCTION_TARGETS['latency']:>5}     | {latency_for_target:>7.2f}    | {'OK' if lat_ok else 'GAP':>6}   |\")\n",
    "print(f\"| Throughput (sps)| >= {PRODUCTION_TARGETS['throughput']:>5}  | {max_throughput:>7.0f}    | {'OK' if thr_ok else 'GAP':>6}   |\")\n",
    "print(f\"| Sensitivity (%) | >= {PRODUCTION_TARGETS['sensitivity']:>5}  | {clinical_results['sensitivity']:>7.2f}    | {'OK' if sen_ok else 'GAP':>6}   |\")\n",
    "print(f\"| Model Size (MB) | <= {PRODUCTION_TARGETS['memory']:>5}  | {model_file_size_mb:>7.1f}    | {'OK' if mem_ok else 'GAP':>6}   |\")\n",
    "print(f\"| FLOP Red.  (%)  | >= {flops_target_reduction:>5}  | {flops_achieved_reduction:>7.1f}    | {'OK' if flp_ok else 'GAP':>6}   |\")\n",
    "print(f\"\\nOverall: {'READY' if all_ok else 'IN PROGRESS'}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Cross-platform deployment analysis\n",
    "\n",
    "We have successfully optimized our model to meet _UdaciMed's Universal Performance Standard_ on our standardized target device. \n",
    "\n",
    "With ONNX, we can easily deploy this optimized model across UdaciMed's diverse hardware fleet just by [changing the Execution Providers](https://onnxruntime.ai/docs/execution-providers/):\n",
    "\n",
    "| Deployment Target\t| Recommended Technology |\tPrimary Goal\t |\tKey Trade-Off | \n",
    "| :--- | :--- | :--- | :--- |\n",
    "| GPU Server (Cloud/On-Prem) |\t\tONNX Runtime + TensorRT\t\t |Max Throughput \t |\tHighest performance vs. more complex setup. | \n",
    "| CPU Workstation (Hospital) |\t\tONNX Runtime + OpenVINO\t\t |Low Latency  |\t\tExcellent CPU speed vs. being tied to Intel hardware. | \n",
    "| Mobile/Edge Device (Clinic) |\t\tONNX Runtime Mobile\t\t | Small Footprint  |\t\tMaximum portability vs. reduced model precision (quantization). | \n",
    "\n",
    "But **what if we need to squeeze out every last drop of performance from each deployment target?** To do this, let's consider moving beyond the portable ONNX format and use specialized, hardware-specific frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.1: Optimization strategy for specialized GPU server deployment**\n",
    "\n",
    "We've established a strong performance baseline using the standard ONNX Runtime with its CUDA Execution Provider (EP).\n",
    "\n",
    "#### GPU deployment options comparison (T4/Tensor‑Core GPU)\n",
    "\n",
    "| Approach | How it runs | FP16 support | Dynamic batching | Expected perf vs ORT(CUDA) | Ops coverage/notes |\n",
    "| :-- | :-- | :--: | :--: | :--: | :-- |\n",
    "| ORT + CUDA EP | ONNX Runtime with CUDAExecutionProvider | Yes | Yes (app‑level) | Baseline | Widest operator coverage; easiest to integrate |\n",
    "| ORT + TensorRT EP | ORT delegates supported subgraphs to TensorRT | Yes (Tensor Cores) | Yes (TensorRT engine) | +1.3–2.0× | Best latency/throughput when subgraph coverage is high; falls back to CUDA EP |\n",
    "| Triton Inference Server (TensorRT backend) | Server hosts TensorRT engines; HTTP/gRPC | Yes | Yes (server‑side scheduler) | +1.3–2.0× (same engine) + batching & multi‑model gains | Adds multi‑model, model‑repo mgmt, autoscaling; network hop adds small latency |\n",
    "\n",
    "Key considerations:\n",
    "- Use ORT+TensorRT EP for maximum single‑process performance when coverage is good; keep ORT CUDA EP as fallback.\n",
    "- Triton adds production features (dynamic batching, model repo, metrics) and can front multiple engines for fleet‑level throughput.\n",
    "\n",
    "#### Recommendation\n",
    "- Export ONNX with FP16 weights allowed and dynamic batch axis.\n",
    "- Prefer ORT session with TensorRT EP (providers=[\"TensorrtExecutionProvider\",\"CUDAExecutionProvider\",\"CPUExecutionProvider\"]).\n",
    "- For large‑scale serving, deploy the TensorRT engine via Triton to leverage server‑side dynamic batching and model management.\n",
    "\n",
    "#### Triton config.pbtxt (FP16 + dynamic batching)\n",
    "```protobuf\n",
    "name: \"pneumonia_detection\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "input [\n",
    "  { name: \"input\", data_type: TYPE_FP16, dims: [ 3, 64, 64 ] }\n",
    "]\n",
    "output [\n",
    "  { name: \"output\", data_type: TYPE_FP16, dims: [ 2 ] }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "# Enable server-side dynamic batching\n",
    "dynamic_batching {\n",
    "  preferred_batch_size: [ 1, 2, 4, 8, 16 ]\n",
    "  max_queue_delay_microseconds: 2000\n",
    "}\n",
    "optimization { execution_accelerators { gpu_execution_accelerator : [ { name : \"tensorrt\" } ] } }\n",
    "```\n",
    "\n",
    "> Note: When ORT uses the TensorRT EP directly, configure the ORT session options; when serving with Triton, generate a TensorRT plan and place it under the model repository with the above config.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.2: Optimization strategy for specialized CPU deployment**\n",
    "\n",
    "#### CPU deployment options comparison (hospital workstation)\n",
    "\n",
    "| Approach | How it runs | Precision | Dynamic batching | Expected perf | Notes |\n",
    "| :-- | :-- | :--: | :--: | :--: | :-- |\n",
    "| ORT + CPU EP | ONNX Runtime with CPUExecutionProvider | FP32 | App‑level | Baseline | Good portability; simplest integration |\n",
    "| OpenVINO (ORT OpenVINO EP or native runtime) | Intel CPU (MKL‑DNN), graph optimizations | FP32/INT8 | Yes (native) | +1.2–2.0× | Best on Intel CPUs; quantization can boost throughput with careful validation |\n",
    "| Triton (OpenVINO backend) | Server hosts OpenVINO model | FP32/INT8 | Yes (server‑side) | Similar to native OpenVINO | Adds model repo, batching, metrics; small RPC overhead |\n",
    "\n",
    "Key CPU considerations:\n",
    "- FP32 keeps numerical stability; INT8/quantization is optional and must be re‑validated for sensitivity.\n",
    "- Set thread counts and memory/pinning for predictable latency under load.\n",
    "\n",
    "#### Recommendation\n",
    "- Use ONNX Runtime with OpenVINO EP (or native OpenVINO runtime) for best single‑host performance while keeping FP32 for clinical stability.\n",
    "- Enable server‑side dynamic batching when fronted by Triton for screening workloads.\n",
    "- For real‑time single‑patient use, fix batch=1 and minimize thread oversubscription.\n",
    "\n",
    "#### Example OpenVINO deployment configuration (YAML)\n",
    "```yaml\n",
    "model: pneumonia_detection\n",
    "precision: FP32         # Keep FP32 for numerical stability; INT8 requires full re‑validation\n",
    "plugin_config:\n",
    "  CPU_THREADS_NUM: 4    # Pin 4 CPU threads for predictable latency\n",
    "  CPU_BIND_THREAD: YES  # Bind threads to cores for consistent timings\n",
    "  CPU_THROUGHPUT_STREAMS: 4  # Parallel streams for throughput scenario\n",
    "  ENFORCE_BF16: NO      # Disable BF16 unless verified across fleet\n",
    "runtime:\n",
    "  batching:\n",
    "    dynamic: true\n",
    "    preferred_batch_sizes: [1, 2, 4, 8]\n",
    "    max_queue_delay_us: 2000\n",
    "validation:\n",
    "  threshold: 0.7        # Clinical operating point maintaining ≥98% sensitivity\n",
    "  sample_rate: full     # Evaluate full test set before promotion\n",
    "```\n",
    "\n",
    "Justification:\n",
    "- **Precision**: FP32 keeps sensitivity stable; consider INT8 only after post‑training calibration with sensitivity audits.\n",
    "- **Threads/streams**: 4 threads + 4 streams strikes a balance between throughput and tail latency on 8–16 core CPUs.\n",
    "- **Dynamic batching**: preferred sizes match our benchmark sweet spots; small queue delay preserves interactivity.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.3: Optimization strategy for mobile and edge deployment**\n",
    "\n",
    "UdaciMed's vision extends beyond hospital workstations to portable devices and mobile health applications. This enables pneumonia detection in rural clinics, emergency response, and preventive screening programs where traditional infrastructure is limited.\n",
    "\n",
    "> **Mobile and edge requirements**: These deployments require lightweight runtimes, offline capability, extended battery life, and often benefit from platform-specific optimizations. However, conversion complexity and clinical validation requirements vary significantly across approaches.\n",
    "\n",
    "#### TODO: Analyze mobile deployment options\n",
    "\n",
    "For mobile, the choice between a cross-platform solution and a native, OS-specific framework is the most critical decision, with significant long-term consequences for development and user experience.\n",
    "\n",
    "Here, the primary constraints are not raw speed, but model size, power consumption, and offline capability. We need a model that is small, efficient, and fully self-contained.\n",
    "\n",
    "_<\\<Complete the table below by filling in missing performance expectations\\>>_\n",
    "\n",
    "| Platform | How it Works | Key Strength | Main Trade-Off | UdaciMed Suitability |\n",
    "|----------|----------------|------------|---------------|-------------------|\n",
    "| **ONNX Runtime Mobile** | A cross-platform engine runs a single ONNX file on iOS & Android. | Portability & simplicity | Not the most optimized performance\t | Best for a fast, low-budget launch to reach all users. |\n",
    "| **ExecuTorch** |  |  |  |  |\n",
    "| **LiteRT** |  |  |  |  |\n",
    "| **Core ML (iOS)** |  |  |  |  |\n",
    "\n",
    "_<\\<Answer the questions below based on UdaciMed's mobile and edge deployment strategy>>_\n",
    "\n",
    "**1. What is the key trade-off between ONNX Runtime Mobile's \"simplicity\" and LiteRT's \"smallest size & fastest speed\"?**\n",
    "<br>_HINT: Think of simplicity vs performance._\n",
    "\n",
    "**2. Which frameworks are best suited for a fully offline-capable app for use in rural clinics with no internet, and why?**\n",
    "<br>_HINT: Think about runtime._\n",
    "\n",
    "**3. For a battery-powered portable device, which frameworks would likely offer the best power efficiency, and what is the trade-off?**\n",
    "<br>_HINT: Think about the benefits of specialized accelerations._\n",
    "\n",
    "#### TODO: Make your strategic choice\n",
    "\n",
    "Based on your analysis, choose the best mobile deployment approach for UdaciMed's initial launch.\n",
    "\n",
    "**My recommendation for UdaciMed's mobile and edge deployment strategy:**\n",
    "\n",
    "_<\\<Choose one approach and justify your decision in 1-2 sentences, considering clinical risk, development resources, and global health reach>>_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!**\n",
    "\n",
    "You have successfully implemented a complete hardware-accelerated deployment pipeline! Let's recap the decisions you have made and results you have achieved while transforming an optimized model into a production-ready healthcare solution.\n",
    "\n",
    "### **TODO: Production deployment scorecard**\n",
    "\n",
    "**Final GPU deployment performance vs UdaciMed targets:**\n",
    "\n",
    "_<\\<Complete final scorecard based on your benchmarking results:>>_\n",
    "\n",
    "| Metric | Target | Achieved | Status |\n",
    "|--------|--------|----------|--------|\n",
    "| **Memory Usage** | <100MB | | |\n",
    "| **Throughput** | >2,000 samples/sec | | |\n",
    "| **Latency** | <3ms | | | |\n",
    "| **FLOP Reduction** | <0.4 GFLOPs per sample | | | |\n",
    "| **Clinical Safety** | >98% sensitivity | | | |\n",
    "\n",
    "_<\\<Give yourself a final production score given the number of targets met>>_\n",
    "\n",
    "**Overall production score: X/5 targets met!**\n",
    "\n",
    "### **TODO: Strategic deployment insights**\n",
    "\n",
    "_<\\<Reflect on the key decisions you made, and why>>_\n",
    "\n",
    "#### Mixed Precision Strategy\n",
    "**Your FP16/FP32 choice:** # _(FP32, FP16)_\n",
    "\n",
    "**Why you made this decision:**\n",
    "\n",
    "#### Backend Selection\n",
    "**Your ONNX execution provider choice:**  _(CPU EP, CUDA EP TensorRT EP, etc.)_\n",
    "\n",
    "**Why this backend aligned with UdaciMed's requirements:**\n",
    "\n",
    "#### Batching Configuration\n",
    "**Your dynamic batching setup:** # _(preferred batch sizes, queue delay, etc.)_\n",
    "\n",
    "**How this supports diverse clinical deployments:** \n",
    "\n",
    "### Optimization Philosophy\n",
    "**Meeting targets vs maximizing metrics:**\n",
    "\n",
    "_<\\<What did you learn about when to stop optimizing and why?>>_\n",
    "\n",
    "---\n",
    "\n",
    "**You have completed the full journey from architectural optimization to production-ready deployment, demonstrating the technical skills and strategic thinking essential for deploying AI in healthcare. Your UdaciMed pneumonia detection system is now ready to serve hospitals worldwide while maintaining the clinical safety standards that save lives.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7.3: Mobile and Edge deployment strategy**\n",
    "\n",
    "#### Platform comparison (mobile / edge)\n",
    "\n",
    "| Target | Framework | Precision | Acceleration | Expected perf | Deployment notes |\n",
    "| :-- | :-- | :--: | :-- | :--: | :-- |\n",
    "| Android | TensorFlow Lite (TFLite) | FP16/INT8 | NNAPI, GPU delegate | 1.2–3× vs FP32 CPU | Best ecosystem support; INT8 requires calibration and sensitivity re‑validation |\n",
    "| iOS | Core ML (via coremltools) | FP16/INT8 | ANE, Metal | 1.5–3× vs FP32 CPU | Strong on‑device acceleration; strict signing and review process |\n",
    "| Edge Linux | ONNX Runtime + OpenVINO (Intel) | FP32/INT8 | OpenVINO EP | 1.2–2× | Good for clinics with Intel NUC/PCs; supports batching |\n",
    "| Edge Linux | ONNX Runtime + TensorRT (Jetson) | FP16/INT8 | TensorRT | 1.5–3× | Good for NVIDIA Jetson deployments; adhere to memory limits |\n",
    "\n",
    "#### Considerations\n",
    "- **Clinical risk**: On‑device inference reduces network latency and data exposure; however, update/recall processes must be robust (signed models, remote kill‑switch).\n",
    "- **Validation**: Any change in precision (FP16/INT8), delegate, or platform requires a regression of sensitivity ≥98%.\n",
    "- **Model size**: Favor depthwise separable backbones and native 64–128 px inputs to meet storage and memory caps on low‑end devices.\n",
    "- **Privacy**: On‑device preprocessing; encrypt model files at rest; avoid PHI on device beyond temporary buffers.\n",
    "\n",
    "#### Recommended plan for UdaciMed\n",
    "- **Android first** with TFLite FP16 path (preserves accuracy, strong device coverage). Quantize to INT8 only after calibration sets prove sensitivity parity.\n",
    "- **iOS second** using Core ML FP16 (ANE/Metal) with the same threshold and sensitivity checklists.\n",
    "- **Edge clinics**: two flavors based on hardware inventory:\n",
    "  - Intel PCs → ORT OpenVINO EP (FP32; INT8 optional with sensitivity audits).\n",
    "  - NVIDIA Jetson → ORT TensorRT EP (FP16) with fixed batch=1 for real‑time.\n",
    "\n",
    "##### Example export pipeline (Android)\n",
    "1. Export to ONNX (already done).\n",
    "2. Convert ONNX → TFLite (via onnx-tf or re‑export TF graph), then tflite_convert with `--enable_select_tf_ops` if needed.\n",
    "3. Enable NNAPI/GPU delegate at runtime; set `allow_fp16_precision_for_fp32=True`.\n",
    "4. Validate sensitivity on‑device against a held‑out test pack before release.\n",
    "\n",
    "##### Example export pipeline (iOS)\n",
    "1. Export to ONNX/TF.\n",
    "2. Convert to Core ML with coremltools, set `precision=fp16`.\n",
    "3. Validate with on‑device tests; use TestFlight for staged rollout.\n",
    "\n",
    "This staged plan balances clinical safety, engineering effort, and global reach while minimizing regulatory surprises for UdaciMed’s mobile program.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
